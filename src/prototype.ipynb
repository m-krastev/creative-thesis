{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Towards Evaluating Creativity in Language\n",
    "\n",
    "This notebook is a prototype for evaluating creativity in language. It is based on the paper [Towards Evaluating Creativity in Language](https://arxiv.org/abs/1904.09751) by [Rudinger et al.](https://arxiv.org/abs/1904.09751) (2019).\n",
    "^ bruh copilot at it again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "import numpy.typing as ntp\n",
    "from madhatter.benchmark import *\n",
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# from classes.models import *\n",
    "# Initialize models\n",
    "import torch\n",
    "from transformers import AutoTokenizer, BertForMaskedLM\n",
    "import gensim\n",
    "from nltk.data import find\n",
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    str(find('models/word2vec_sample/pruned.word2vec.txt')), binary=False)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "bench = CreativityBenchmark(gutenberg.raw(\"austen-emma.txt\"), \"Emma\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report took ~0.317s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Report(title='Emma', nwords=185438, mean_wl=3.919380062338895, mean_sl=117.23189377682404, mean_tokenspersent=25.721968884120173, prop_contentwords=0.6073728146334624, mean_conc=2.641153251837003, mean_img=3.071953538216892, prop_pos={'OTHER': 0.5883420324012034, 'NOUN': 0.16685524785825648, 'VERB': 0.18625738464827435, 'ADJ': 0.05854533509226574})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bench.report(postag_distribution=True)\n",
    "# del CreativityBenchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potential creativity measures\n",
    "#### Usage of less common vocabulary uncommon words\n",
    "**Example:**\n",
    "- \"The quick brown fox jumps over the lazy dog.\"\n",
    "- \"The swift hazel-furred fox leaps over the idle dog.\"\n",
    "\n",
    "**Idea:**\n",
    "\n",
    "Adapt resources such as WordNet for finding semantically similar words and compare them to their most used synonym. Notion: The more uncommon the word, the more creative the sentence.\n",
    "Potential problems: words could be too far off from their actual meaning in the context. For example, here \"leaps\" is a synonym for \"jumps\", but using the word \"vaults\" or \"springs\" might not fit the context.\n",
    "\n",
    "\n",
    "#### Comparing alternatives for bi(+)grams \n",
    "(Generally, we want to narrow down on adjectives and noun phrases, but this could be expanded to verb phrases for example.)  \n",
    "**Two variants:**\n",
    "1. Compare how much the original word deviates in comparison to contextual synonyms/alternatives. That is, compare $ P(w_{original}|context) $ with $ \\{P(w| context) | w \\text{ in the set of alternative continuations}\\} $. **This is somewhat akin to the perplexity measure, I believe?**\n",
    "   - *Example:* (The following has been generated by Copilot) Given the sentence \"The quick brown fox jumps over the lazy dog.\", the context word is \"fox\" and the alternative contexts are \"dog\" and \"cat\". The probability of \"jumps\" given \"fox\" is compared to the probabilities of \"jumps\" given \"dog\" and \"cat\". \n",
    "   - **Alternatively**, to simplify the formulas, we can compare the deviation of probability $ P(w_{original}|context) $ with respect to the likeliest/largest/maximum element in the set of probability distribution described above, i.e. $\\max (\\{ P(w|context) | w \\in S_{Alternatives}\\})$ \n",
    "\n",
    "\n",
    "2. Compare the deviation of probability $ P(word|modifier) $ with respect to the set $ \\{P(word| alt) | alt \\text{ in the set of alternative modifiers}\\}$\n",
    "   - *Example:* Given the sentence \"The quick *brown* **fox** jumps over the lazy dog.\", the $word$ is **\"fox\"** and the $modifier$ is *\"brown\"*. Then, the alternative modifiers can be \"black\", \"reddish\", or even \"blue\".\n",
    "   - This can be summarised by doing evaluation on the noun phrase level. I'd personally prefer focusing on the prepositional modifier words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible algorithm for measuring number of tokens per sentence:\n",
    "1. Split the text into sentences using the sentence tokenizer.\n",
    "2. For each sentence, split it into tokens using the word tokenizer.\n",
    "\n",
    "Additionally, plot the distribution of the number of tokens per sentence.\n",
    "Additionally, plot the distribution of different PoS tags per sentence.\n",
    "Do this for a few genres and compare them. PLOT PLOTS PLOTS\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance measure based on semantic tree traversal\n",
    "**Idea:**\n",
    "\n",
    "- Given a sentence, tag words into parts of speech using the Universal tagset (we prefer not to use the PennTreebank tagset as it is too English-specific and would not mesh well with WordNet).\n",
    "- Filter only to nouns, adjectives, verbs, and adverbs. \n",
    "- Given each tagged word, we find its synset (i.e. the set of synonyms) in WordNet.\n",
    "- Compute some distance metric between the synsets of the two words. For example, we can use the [Wu-Palmer similarity](https://www.nltk.org/howto/wordnet.html) measure.\n",
    "- How do we calculate that for all the words in a given text/sentence? \n",
    "\n",
    "#### Potential Issues\n",
    "Potential issues with this approach may include:\n",
    "- The use of WordNet. It is a good resource, but it is not perfect. For example, it does not contain all the words in the English language.\n",
    "- Setting. For example, some words may not be subsititutable in certain context. Say, the collocation \"big sister\" cannot be replaced with \"large sister\" or \"huge sister\". It completely alters the meaning of the phrase. For example, the word H2O is used in scientific contexts and would be inappropriate in a hiking guide—water would be more appropriate— and this genre difference is part of the meaning of the word. In practice, the word syn- onym is therefore used to describe a relationship of approximate or rough synonymy. \\cite{jurafsky2014speechorwhatever}\n",
    "- \n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warming up PyWSD (takes ~10 secs)... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quick - ADJ - Synset('agile.s.01') - 0.07056773453950882 - moving quickly and lightly\n",
      "quick - ADJ - Synset('agile.s.01') - 0.07056773453950882 - moving quickly and lightly\n",
      "brown - NOUN - Synset('brown_university.n.01') - 0.12424251437187195 - a university in Rhode Island\n",
      "brown - NOUN - Synset('brown.n.01') - 0.12424251437187195 - an orange of low brightness and saturation\n",
      "fox - NOUN - Synset('fox.n.07') - 0.31961506605148315 - the Algonquian language of the Fox\n",
      "fox - NOUN - Synset('fox.n.03') - 0.31961506605148315 - the grey or reddish-brown fur of a fox\n",
      "jumps - VERB - Synset('startle.v.02') - 0.0884588360786438 - move or jump suddenly, as if in surprise or alarm\n",
      "jumps - VERB - Synset('jump.v.06') - 0.0884588360786438 - enter eagerly into\n",
      "lazy - ADJ - Synset('lazy.s.01') - 0.05496400594711304 - moving slowly and gently\n",
      "lazy - ADJ - Synset('lazy.s.01') - 0.05496400594711304 - moving slowly and gently\n",
      "dog - NOUN - Synset('pawl.n.01') - 0.6918289065361023 - a hinged catch that fits into a notch of a ratchet to move a wheel forward or prevent it from moving backward\n",
      "dog - NOUN - Synset('dog.n.01') - 0.6918289065361023 - a member of the genus Canis (probably descended from the common wolf) that has been domesticated by man since prehistoric times; occurs in many breeds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "took 3.680233955383301 secs.\n"
     ]
    }
   ],
   "source": [
    "# This implementation is shit, aye?\n",
    "from nltk import wsd\n",
    "from nltk.corpus import wordnet as wn\n",
    "import pywsd\n",
    "\n",
    "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
    "words = nltk.word_tokenize(sentence)\n",
    "tagged: list[tuple[str, str]] = nltk.pos_tag(words, tagset='universal')\n",
    "universal_to_wn = {'NOUN': wn.NOUN,\n",
    "                   'VERB': wn.VERB, 'ADJ': wn.ADJ}\n",
    "for word, tag in tagged:\n",
    "    if tag not in universal_to_wn.keys():\n",
    "        continue\n",
    "    actual_word = pywsd.simple_lesk(sentence, word, universal_to_wn[tag])\n",
    "    for method in [pywsd.simple_lesk, pywsd.cosine_lesk]:\n",
    "        actual_word = method(sentence, word, universal_to_wn[tag])\n",
    "        print(\n",
    "            f\"{word} - {tag} - {actual_word} - {word2vec_model.similarity(word, 'canine')} - {actual_word.definition() if actual_word else ''}\")\n",
    "    # print(\n",
    "    #     f\"{word} - {tag} - {actual_word, actual_word.definition() if actual_word else ''} - {wn.synsets(word, pos=universal_to_wn[tag])}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot average concreteness of words in a sentence\n",
    "\n",
    "^^ \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative implementation for semantic distance measure which abuses masked language models for context-aware words\n",
    "\n",
    "Initialize models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issues with Word2Vec\n",
    "Does not quite capture part of speech senses.\n",
    "\n",
    "**Remarks:**\n",
    "1. Cosine similarity returns values in the range of $[0,1]$. The closer the value is to 1, the more similar the two vectors are. Although intuitively, a cosine function can range between $[-1, -1]$, the learned embeddings being used in `Word2Vec` themselves can inherently only have values in the range of $[0,1]$, so we cannot make assumptions for negative values, such as `Word_A` and `Word_B` are antonyms."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Benchmarking insights for speed concerns\n",
    "Torch objects and NumPy arrays have effectively the same speed. However, NumPy arrays are more memory efficient."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slope of the curve of likelihood of a word given a context\n",
    "<!-- We explore how certainty of a word given a context changes as we move away from the context. (this is not a bad suggestion by copilot, but it strays from my original purpose for this project) -->\n",
    "We explore how certainty for a prediction given some context changes in the likelihood space of the BERT MLM. In practice, this enables us to see how certain the model is about the predictions it is making, and it can potentially allow us to compare values across different contexts or same contexts but with different masked tokens.\n",
    "\n",
    "The slope of a given curve has the following equation(where $x$ is the input and $y$ is the output):\n",
    "$$\n",
    "\\sum_{i = 1} ^ n \\frac{(x_i - \\bar{x})(y_i - \\bar{y})}{(x_i - \\bar{x}) ^ 2}\n",
    "\n",
    "$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 100 sentences of Brown corpus (~1000 tokens) need 1.5 minutes to be processed. Not based at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_predictions(sent: str | list[str], model: Any, return_tokens: bool = False, k: int = 20):\n",
    "    \"\"\"Returns predictions for content words in a given sentence. If return_tokens is true, returns a key-value pair dictionary where the key is the used word, and the value is a list of suggested tokens, corresponding to the likekihoods in the first list.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sent : str | list[str]\n",
    "        The sentence to be used for evaluation. It can be either a list of words or a normal string.\n",
    "    model : BertForMaskedLM\n",
    "        _description_\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[NDArray]\n",
    "        A list of Nx2 matrices where the first column is the likelihood of a given fill and the second column is the word2vec similarity with the masked word.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    TypeError\n",
    "        When the sentence is in the wrong type\n",
    "    \"\"\"\n",
    "    if isinstance(sent, str):\n",
    "        pass\n",
    "    elif isinstance(sent, list):\n",
    "        tokens = [token.lower() for token in sent]\n",
    "        sent = \" \".join(tokens)\n",
    "    else:\n",
    "        raise TypeError()\n",
    "    words = nltk.pos_tag(sent, tagset='universal')\n",
    "\n",
    "    results = []\n",
    "    return_words = {}\n",
    "\n",
    "    # loop over the words of the sentence\n",
    "    for word, tag in words:\n",
    "        # Early stopping\n",
    "        if word in stopwords or tag not in bench.tags_of_interest:\n",
    "            continue\n",
    "\n",
    "        if return_tokens == True:\n",
    "            predicted_tokens, predicted_words = predict_tokens(\n",
    "                sent, word, model, return_tokens=return_tokens, k=k)\n",
    "\n",
    "            results.append(predicted_tokens)\n",
    "            return_words[(word, tag)] = predicted_words\n",
    "        else:\n",
    "            predicted_tokens = predict_tokens(\n",
    "                sent, word, model, return_tokens=return_tokens, k=k)\n",
    "            results.append(predicted_tokens)\n",
    "\n",
    "    if return_tokens == True:\n",
    "\n",
    "        return [np.array(result) for result in results], return_words\n",
    "    else:\n",
    "        return [np.array(result) for result in results]\n",
    "\n",
    "\n",
    "def calculate_sent_slopes(bench, model, n) -> list[list[float]]:\n",
    "    # Returns slopes for the __words__ of the first `n` sentences of the `sents` list of sentences.\n",
    "    res = []\n",
    "    for sent in bench.tokenized_sents[:n]:\n",
    "        results = sent_predictions(sent, model)\n",
    "\n",
    "        res.append(\n",
    "            [slope_coefficient(\n",
    "                np.arange(len(result)),\n",
    "                result)\n",
    "             for result in results\n",
    "             if len(result) > 0]\n",
    "        )\n",
    "\n",
    "    return res\n",
    "\n",
    "# n = 10\n",
    "# res = calculate_sent_slopes(bench, model, n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tokens: expected a list of strings, got a string",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 58\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     55\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m calculate_sim_scores(bench, sim_func, \u001b[39m10\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m, in \u001b[0;36mcalculate_sim_scores\u001b[0;34m(bench, sim_function, max_sents)\u001b[0m\n\u001b[1;32m      2\u001b[0m similarity_scores \u001b[39m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m bench\u001b[39m.\u001b[39mtokenized_sents[:max_sents]:\n\u001b[0;32m----> 5\u001b[0m     probs, predictions \u001b[39m=\u001b[39m sent_predictions(\n\u001b[1;32m      6\u001b[0m         sent, model, return_tokens\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, k\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n\u001b[1;32m      7\u001b[0m     average_position_of_correct_prediction \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      8\u001b[0m     \u001b[39m# number of predictions which do not include the true value in the topmost k results\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 28\u001b[0m, in \u001b[0;36msent_predictions\u001b[0;34m(sent, model, return_tokens, k)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     27\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m()\n\u001b[0;32m---> 28\u001b[0m words \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mpos_tag(sent, tagset\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39muniversal\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     30\u001b[0m results \u001b[39m=\u001b[39m []\n\u001b[1;32m     31\u001b[0m return_words \u001b[39m=\u001b[39m {}\n",
      "File \u001b[0;32m~/miniconda3/envs/mlkit/lib/python3.10/site-packages/nltk/tag/__init__.py:166\u001b[0m, in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[39mUse NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39mtag the given list of tokens.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[39m:rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    165\u001b[0m tagger \u001b[39m=\u001b[39m _get_tagger(lang)\n\u001b[0;32m--> 166\u001b[0m \u001b[39mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
      "File \u001b[0;32m~/miniconda3/envs/mlkit/lib/python3.10/site-packages/nltk/tag/__init__.py:120\u001b[0m, in \u001b[0;36m_pos_tag\u001b[0;34m(tokens, tagset, tagger, lang)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39m# Throws Error if tokens is of string type\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(tokens, \u001b[39mstr\u001b[39m):\n\u001b[0;32m--> 120\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mtokens: expected a list of strings, got a string\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    122\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    123\u001b[0m     tagged_tokens \u001b[39m=\u001b[39m tagger\u001b[39m.\u001b[39mtag(tokens)\n",
      "\u001b[0;31mTypeError\u001b[0m: tokens: expected a list of strings, got a string"
     ]
    }
   ],
   "source": [
    "from typing import Callable\n",
    "\n",
    "\n",
    "def calculate_sim_scores(bench, sim_function: Callable, max_sents=-1):\n",
    "    similarity_scores = []\n",
    "    for sent in bench.tokenized_sents[:max_sents]:\n",
    "\n",
    "        probs, predictions = sent_predictions(\n",
    "            sent, model, return_tokens=True, k=10)\n",
    "        average_position_of_correct_prediction = 0\n",
    "        # number of predictions which do not include the true value in the topmost k results\n",
    "        missed_predictions = 0\n",
    "        # note that word here is a tuple of the word and its POS tag\n",
    "        i = 0\n",
    "        for (word, predlist) in predictions.items():\n",
    "            try:\n",
    "                # print(word[0], predlist)\n",
    "                average_position_of_correct_prediction += predlist.index(\n",
    "                    word[0])\n",
    "                i += 1\n",
    "            except ValueError:\n",
    "                missed_predictions += 1\n",
    "\n",
    "        # Avoid division by zero error\n",
    "        if i == 0:\n",
    "            average_position_of_correct_prediction = None\n",
    "        else:\n",
    "            average_position_of_correct_prediction /= i\n",
    "        similarity_scores.append(\n",
    "            (average_position_of_correct_prediction, missed_predictions))\n",
    "        break\n",
    "        #     for item in predlist:\n",
    "        # similarity_scores.append(\n",
    "        #     [[sim_function(word[0], pred) for pred in predlist] for word,predlist in predictions.items()]\n",
    "        # )\n",
    "\n",
    "    return similarity_scores\n",
    "\n",
    "\n",
    "def sim_func(word: str, pred: str) -> float | None:\n",
    "    \"\"\"Arbitrary function to use when calculating vector similarity between the embeddings of two words. Serves as an example.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    word : str\n",
    "        Normally, the original (true) value.\n",
    "    pred : str\n",
    "        Normally, the predicted value.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Optional[float]\n",
    "        Can return a float or None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return word2vec_model.similarity(word, pred)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "calculate_sim_scores(bench, sim_func, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tokens: expected a list of strings, got a string",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m res \u001b[39m=\u001b[39m calculate_sent_slopes(bench, model, \u001b[39m10\u001b[39;49m)\n\u001b[1;32m      2\u001b[0m \u001b[39m# flatten the list\u001b[39;00m\n\u001b[1;32m      3\u001b[0m plotting_list \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([item \u001b[39mfor\u001b[39;00m sublist \u001b[39min\u001b[39;00m res \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m sublist])\n",
      "Cell \u001b[0;32mIn[3], line 61\u001b[0m, in \u001b[0;36mcalculate_sent_slopes\u001b[0;34m(bench, model, n)\u001b[0m\n\u001b[1;32m     59\u001b[0m res \u001b[39m=\u001b[39m []\n\u001b[1;32m     60\u001b[0m \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m bench\u001b[39m.\u001b[39mtokenized_sents[:n]:\n\u001b[0;32m---> 61\u001b[0m     results \u001b[39m=\u001b[39m sent_predictions(sent, model)\n\u001b[1;32m     63\u001b[0m     res\u001b[39m.\u001b[39mappend(\n\u001b[1;32m     64\u001b[0m         [slope_coefficient(\n\u001b[1;32m     65\u001b[0m             np\u001b[39m.\u001b[39marange(\u001b[39mlen\u001b[39m(result)),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     68\u001b[0m          \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(result) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m]\n\u001b[1;32m     69\u001b[0m     )\n\u001b[1;32m     71\u001b[0m \u001b[39mreturn\u001b[39;00m res\n",
      "Cell \u001b[0;32mIn[3], line 28\u001b[0m, in \u001b[0;36msent_predictions\u001b[0;34m(sent, model, return_tokens, k)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     27\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m()\n\u001b[0;32m---> 28\u001b[0m words \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mpos_tag(sent, tagset\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39muniversal\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     30\u001b[0m results \u001b[39m=\u001b[39m []\n\u001b[1;32m     31\u001b[0m return_words \u001b[39m=\u001b[39m {}\n",
      "File \u001b[0;32m~/miniconda3/envs/mlkit/lib/python3.10/site-packages/nltk/tag/__init__.py:166\u001b[0m, in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[39mUse NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39mtag the given list of tokens.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[39m:rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    165\u001b[0m tagger \u001b[39m=\u001b[39m _get_tagger(lang)\n\u001b[0;32m--> 166\u001b[0m \u001b[39mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
      "File \u001b[0;32m~/miniconda3/envs/mlkit/lib/python3.10/site-packages/nltk/tag/__init__.py:120\u001b[0m, in \u001b[0;36m_pos_tag\u001b[0;34m(tokens, tagset, tagger, lang)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39m# Throws Error if tokens is of string type\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(tokens, \u001b[39mstr\u001b[39m):\n\u001b[0;32m--> 120\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mtokens: expected a list of strings, got a string\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    122\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    123\u001b[0m     tagged_tokens \u001b[39m=\u001b[39m tagger\u001b[39m.\u001b[39mtag(tokens)\n",
      "\u001b[0;31mTypeError\u001b[0m: tokens: expected a list of strings, got a string"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "res = calculate_sent_slopes(bench, model, n)\n",
    "# flatten the list\n",
    "plotting_list = np.array([item for sublist in res for item in sublist])\n",
    "# invert likelihood\n",
    "plotting_list = -plotting_list\n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.plot(plotting_list)\n",
    "plt.title(\n",
    "    f\"Aggregate predictability of content words within the context of first {n} sentences (Mean: {plotting_list.mean():.3f})\")\n",
    "plt.xlabel('Word #')\n",
    "plt.ylabel('Predictability')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similar idea, but sentence-level instead of word-level\n",
    "Use `BertForNextSentencePrediction` to determine how \"unpredictable\" the following sentence is given the previous sentence (as prompt). We mask the second sentence and take something like a cosine similarity / set difference between the two vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import BertForNextSentencePrediction\n",
    "model = BertForNextSentencePrediction.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\n",
    "next_sentence = \"The sky is blue due to the shorter wavelength of blue light.\"\n",
    "\n",
    "encoding = tokenizer(prompt, next_sentence, return_tensors=\"pt\")\n",
    "outputs = model(**encoding, labels=torch.LongTensor([1]))\n",
    "\n",
    "logits = outputs.logits\n",
    "print(logits[0, 0], logits[0, 1])  # next sentence was random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "- What current approaches to creativity evaluation have been implemented so far?\n",
    "\n",
    "### Related Work\n",
    "- Current methods/similar work with POS tags, proof that POS tags are (not) enough to capture creativity\n",
    "\n",
    "\n",
    "- Putting some ideas of the notebook into the thesis\n",
    "- Write down some methodology or datasets \n",
    "- Write down some datasets for the experiment\n",
    "- Draw some comparisons between the suggested replacements in the masked model - maybe mean, cosine similarity, etc.\n",
    "- Maybe use stuff like word2vec to show differences between the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7408115070746587, 0.5700001)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import wordnet_ic as ic\n",
    "dog = wn.synset('wolf.n.1')\n",
    "fox = wn.synset('fox.n.1')\n",
    "\n",
    "# # dog.common_hypernyms(fox)\n",
    "ic_dict = ic.ic(\"ic-brown.dat\")\n",
    "dog.lin_similarity(fox, ic_dict), word2vec_model.similarity(\"wolf\", \"fox\")\n",
    "\n",
    "# # time = wn.synsets(\"time\")\n",
    "# # for synset in time:\n",
    "# #     print(synset, synset.definition())\n",
    "# # wn.synset(\"time.v.1\").definition()\n",
    "# help(ic)\n",
    "# print(ic_dict[\"v\"])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find average similarity of suggested tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', 'VOLUME', 'I', 'CHAPTER', 'I', 'Emma', 'Woodhouse', ',', 'handsome', ',', 'clever', ',', 'and', 'rich', ',', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', ',', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessings', 'of', 'existence', ';', 'and', 'had', 'lived', 'nearly', 'twenty-one', 'years', 'in', 'the', 'world', 'with', 'very', 'little', 'to', 'distress', 'or', 'vex', 'her', '.']\n",
      "['She', 'was', 'the', 'youngest', 'of', 'the', 'two', 'daughters', 'of', 'a', 'most', 'affectionate', ',', 'indulgent', 'father', ';', 'and', 'had', ',', 'in', 'consequence', 'of', 'her', 'sister', \"'s\", 'marriage', ',', 'been', 'mistress', 'of', 'his', 'house', 'from', 'a', 'very', 'early', 'period', '.']\n",
      "['Her', 'mother', 'had', 'died', 'too', 'long', 'ago', 'for', 'her', 'to', 'have', 'more', 'than', 'an', 'indistinct', 'remembrance', 'of', 'her', 'caresses', ';', 'and', 'her', 'place', 'had', 'been', 'supplied', 'by', 'an', 'excellent', 'woman', 'as', 'governess', ',', 'who', 'had', 'fallen', 'little', 'short', 'of', 'a', 'mother', 'in', 'affection', '.']\n",
      "['Sixteen', 'years', 'had', 'Miss', 'Taylor', 'been', 'in', 'Mr.', 'Woodhouse', \"'s\", 'family', ',', 'less', 'as', 'a', 'governess', 'than', 'a', 'friend', ',', 'very', 'fond', 'of', 'both', 'daughters', ',', 'but', 'particularly', 'of', 'Emma', '.']\n",
      "['Between', '_them_', 'it', 'was', 'more', 'the', 'intimacy', 'of', 'sisters', '.']\n",
      "['Even', 'before', 'Miss', 'Taylor', 'had', 'ceased', 'to', 'hold', 'the', 'nominal', 'office', 'of', 'governess', ',', 'the', 'mildness', 'of', 'her', 'temper', 'had', 'hardly', 'allowed', 'her', 'to', 'impose', 'any', 'restraint', ';', 'and', 'the', 'shadow', 'of', 'authority', 'being', 'now', 'long', 'passed', 'away', ',', 'they', 'had', 'been', 'living', 'together', 'as', 'friend', 'and', 'friend', 'very', 'mutually', 'attached', ',', 'and', 'Emma', 'doing', 'just', 'what', 'she', 'liked', ';', 'highly', 'esteeming', 'Miss', 'Taylor', \"'s\", 'judgment', ',', 'but', 'directed', 'chiefly', 'by', 'her', 'own', '.']\n",
      "['The', 'real', 'evils', ',', 'indeed', ',', 'of', 'Emma', \"'s\", 'situation', 'were', 'the', 'power', 'of', 'having', 'rather', 'too', 'much', 'her', 'own', 'way', ',', 'and', 'a', 'disposition', 'to', 'think', 'a', 'little', 'too', 'well', 'of', 'herself', ';', 'these', 'were', 'the', 'disadvantages', 'which', 'threatened', 'alloy', 'to', 'her', 'many', 'enjoyments', '.']\n",
      "['The', 'danger', ',', 'however', ',', 'was', 'at', 'present', 'so', 'unperceived', ',', 'that', 'they', 'did', 'not', 'by', 'any', 'means', 'rank', 'as', 'misfortunes', 'with', 'her', '.']\n",
      "['Sorrow', 'came', '--', 'a', 'gentle', 'sorrow', '--', 'but', 'not', 'at', 'all', 'in', 'the', 'shape', 'of', 'any', 'disagreeable', 'consciousness.', '--', 'Miss', 'Taylor', 'married', '.']\n",
      "['It', 'was', 'Miss', 'Taylor', \"'s\", 'loss', 'which', 'first', 'brought', 'grief', '.']\n"
     ]
    }
   ],
   "source": [
    "# for sent in bench.tokenized_sents[:10]:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report took ~0.447s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'.',\n",
       " 'ADJ',\n",
       " 'ADP',\n",
       " 'ADV',\n",
       " 'CONJ',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'NUM',\n",
       " 'PRON',\n",
       " 'PRT',\n",
       " 'VERB',\n",
       " 'X'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bench.report(postag_distribution=True)\n",
    "bench.tags"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "dc2ee436cbdf7adb3462c2431d641fae6796d2f28e77e1bb10007c2aae13c49d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
