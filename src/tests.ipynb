{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The memory_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext memory_profiler\n"
     ]
    }
   ],
   "source": [
    "from madhatter import *\n",
    "from nltk.corpus import gutenberg\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import nltk\n",
    "\n",
    "%load_ext memory_profiler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'lemmatizer']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = [gutenberg.raw(fileid)[:100_000] for fileid in gutenberg.fileids()]\n",
    "# files = files*100\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\n",
    "                 \"ner\", \n",
    "                #  \"lemmatizer\", \n",
    "                 \"textcat\", \"attribute_ruler\"])\n",
    "nlp.pipe_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Matey/miniconda3/envs/mlkit/lib/python3.10/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "# %%memit\n",
    "docs = nlp.pipe(files, n_process=-1)\n",
    "for doc in docs:\n",
    "    sent = list(doc.sents)[0]\n",
    "    for token in sent:\n",
    "        # print(token.text, token.tag_, token.pos_, token.dep_, token.is_stop)\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find file <string>\n",
      "work_done-[Poems by William Bl\n",
      "work_done-[Sense and Sensibili\n",
      "work_done-[Persuasion by Jane \n",
      "work_done-[The Adventures of B\n",
      "work_done-[Emma by Jane Austen\n",
      "work_done-[The King James Bibl\n",
      "work_done-[Stories to Tell to \n",
      "work_done-[Alice's Adventures \n",
      "work_done-[The Ball and The Cr\n",
      "work_done-[The Wisdom of Fathe\n",
      "work_done-[The Parent's Assist\n",
      "work_done-[The Man Who Was Thu\n",
      "work_done-[Moby Dick by Herman\n",
      "work_done-[Paradise Lost by Jo\n",
      "work_done-[The Tragedie of Jul\n",
      "work_done-[The Tragedie of Ham\n",
      "work_done-[The Tragedie of Mac\n",
      "work_done-[Leaves of Grass by \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "%%mprun -f x\n",
    "# for file in files:\n",
    "#     bench = CreativityBenchmark(file)\n",
    "#     sent = bench.tagged_sents[0]\n",
    "#     for tup in sent:\n",
    "#         # print(word)\n",
    "#         pass\n",
    "\n",
    "def x():\n",
    "    def do_work(file):\n",
    "        bench = CreativityBenchmark(file)\n",
    "        sent = bench.tagged_sents[0]\n",
    "        for tup in sent:\n",
    "            # print(word)\n",
    "            pass\n",
    "        print(f\"work_done-{file[:20]}\")\n",
    "    \n",
    "    # from multiprocessing import pool\n",
    "    from multiprocess import pool\n",
    "\n",
    "    p = pool.Pool(processes=8)\n",
    "    res = p.map(do_work, files)\n",
    "# with pool.Pool() as p:\n",
    "#     res = p.map(do_work, files)\n",
    "\n",
    "x()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory usage of Spacy vs Custom Package\n",
    "| Framework | peak memory | increment |\n",
    "|  ------ | ---------- | -------- |\n",
    "| Spacy | 5089.13 MiB |  4465.29 MiB |\n",
    "| Mad Hatter| 434.81 MiB  | 48.75 MiB |\n",
    "\n",
    "Increment here is the more important number as it tells us how memory usage peaks when performing a given operation.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlkit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
