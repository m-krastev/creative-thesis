{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Towards Evaluating Creativity in Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report took ~0.347s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BookReport(title='Emma', nwords=185438, mean_wl=3.919380062338895, mean_sl=117.23189377682404, mean_tokenspersent=25.721968884120173, prop_contentwords=0.04020750870911032, mean_conc=2.7062426711716205, mean_img=3.3097397867972163, mean_freq=-2.3673723826953306, prop_pos={'NOUN': 0.16685524785825648, 'VERB': 0.18625738464827435, 'ADJ': 0.05854533509226574}, surprisal=None, predictability=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os.path\n",
    "# Addition to path to unlock relative import to the madhatter package\n",
    "sys.path.append(os.path.abspath(os.path.pardir))\n",
    "\n",
    "from nltk.corpus import gutenberg\n",
    "import numpy.typing as ntp\n",
    "from madhatter.benchmark import *\n",
    "from madhatter.models import *\n",
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize models\n",
    "import torch\n",
    "import gensim\n",
    "from nltk.data import find\n",
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    str(find('models/word2vec_sample/pruned.word2vec.txt')), binary=False)\n",
    "\n",
    "# model, tokenizer = default_model(\"bert-base-uncased\")\n",
    "\n",
    "bench = CreativityBenchmark(gutenberg.raw(\"austen-emma.txt\"), \"Emma\")\n",
    "bench.report(include_pos=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potential creativity measures\n",
    "#### Usage of less common vocabulary uncommon words\n",
    "**Example:**\n",
    "- \"The quick brown fox jumps over the lazy dog.\"\n",
    "- \"The swift hazel-furred fox leaps over the idle dog.\"\n",
    "\n",
    "**Idea:**\n",
    "\n",
    "Adapt resources such as WordNet for finding semantically similar words and compare them to their most used synonym. Notion: The more uncommon the word, the more creative the sentence.\n",
    "Potential problems: words could be too far off from their actual meaning in the context. For example, here \"leaps\" is a synonym for \"jumps\", but using the word \"vaults\" or \"springs\" might not fit the context.\n",
    "\n",
    "\n",
    "#### Comparing alternatives for bi(+)grams \n",
    "(Generally, we want to narrow down on adjectives and noun phrases, but this could be expanded to verb phrases for example.)  \n",
    "**Two variants:**\n",
    "1. Compare how much the original word deviates in comparison to contextual synonyms/alternatives. That is, compare $ P(w_{original}|context) $ with $ \\{P(w| context) | w \\text{ in the set of alternative continuations}\\} $. **This is somewhat akin to the perplexity measure, I believe?**\n",
    "   - *Example:* (The following has been generated by Copilot) Given the sentence \"The quick brown fox jumps over the lazy dog.\", the context word is \"fox\" and the alternative contexts are \"dog\" and \"cat\". The probability of \"jumps\" given \"fox\" is compared to the probabilities of \"jumps\" given \"dog\" and \"cat\". \n",
    "   - **Alternatively**, to simplify the formulas, we can compare the deviation of probability $ P(w_{original}|context) $ with respect to the likeliest/largest/maximum element in the set of probability distribution described above, i.e. $\\max (\\{ P(w|context) | w \\in S_{Alternatives}\\})$ \n",
    "\n",
    "\n",
    "2. Compare the deviation of probability $ P(word|modifier) $ with respect to the set $ \\{P(word| alt) | alt \\text{ in the set of alternative modifiers}\\}$\n",
    "   - *Example:* Given the sentence \"The quick *brown* **fox** jumps over the lazy dog.\", the $word$ is **\"fox\"** and the $modifier$ is *\"brown\"*. Then, the alternative modifiers can be \"black\", \"reddish\", or even \"blue\".\n",
    "   - This can be summarised by doing evaluation on the noun phrase level. I'd personally prefer focusing on the prepositional modifier words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible algorithm for measuring number of tokens per sentence:\n",
    "1. Split the text into sentences using the sentence tokenizer.\n",
    "2. For each sentence, split it into tokens using the word tokenizer.\n",
    "\n",
    "Additionally, plot the distribution of the number of tokens per sentence.\n",
    "Additionally, plot the distribution of different PoS tags per sentence.\n",
    "Do this for a few genres and compare them. PLOT PLOTS PLOTS\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance measure based on semantic tree traversal\n",
    "**Idea:**\n",
    "\n",
    "- Given a sentence, tag words into parts of speech using the Universal tagset (we prefer not to use the PennTreebank tagset as it is too English-specific and would not mesh well with WordNet).\n",
    "- Filter only to nouns, adjectives, verbs, and adverbs. \n",
    "- Given each tagged word, we find its synset (i.e. the set of synonyms) in WordNet.\n",
    "- Compute some distance metric between the synsets of the two words. For example, we can use the [Wu-Palmer similarity](https://www.nltk.org/howto/wordnet.html) measure.\n",
    "- How do we calculate that for all the words in a given text/sentence? \n",
    "\n",
    "#### Potential Issues\n",
    "Potential issues with this approach may include:\n",
    "- The use of WordNet. It is a good resource, but it is not perfect. For example, it does not contain all the words in the English language.\n",
    "- Setting. For example, some words may not be subsititutable in certain context. Say, the collocation \"big sister\" cannot be replaced with \"large sister\" or \"huge sister\". It completely alters the meaning of the phrase. For example, the word H2O is used in scientific contexts and would be inappropriate in a hiking guide—water would be more appropriate— and this genre difference is part of the meaning of the word. In practice, the word syn- onym is therefore used to describe a relationship of approximate or rough synonymy. \\cite{jurafsky2014speechorwhatever}\n",
    "- \n",
    "- "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issues with Word2Vec\n",
    "Does not quite capture part of speech senses.\n",
    "\n",
    "**Remarks:**\n",
    "1. Cosine similarity returns values in the range of $[0,1]$. The closer the value is to 1, the more similar the two vectors are. Although intuitively, a cosine function can range between $[-1, -1]$, the learned embeddings being used in `Word2Vec` themselves can inherently only have values in the range of $[0,1]$, so we cannot make assumptions for negative values, such as `Word_A` and `Word_B` are antonyms."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Benchmarking insights for speed concerns\n",
    "Torch objects and NumPy arrays have effectively the same speed. However, NumPy arrays are more memory efficient."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slope of the curve of likelihood of a word given a context\n",
    "<!-- We explore how certainty of a word given a context changes as we move away from the context. (this is not a bad suggestion by copilot, but it strays from my original purpose for this project) -->\n",
    "We explore how certainty for a prediction given some context changes in the likelihood space of the BERT MLM. In practice, this enables us to see how certain the model is about the predictions it is making, and it can potentially allow us to compare values across different contexts or same contexts but with different masked tokens.\n",
    "\n",
    "The slope of a given curve has the following equation(where $x$ is the input and $y$ is the output):\n",
    "$$\n",
    "\\sum_{i = 1} ^ n \\frac{(x_i - \\bar{x})(y_i - \\bar{y})}{(x_i - \\bar{x}) ^ 2}\n",
    "\n",
    "$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 100 sentences of Brown corpus (~1000 tokens) need 1.5 minutes to be processed. Not based at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similar idea, but sentence-level instead of word-level\n",
    "Use `BertForNextSentencePrediction` to determine how \"unpredictable\" the following sentence is given the previous sentence (as prompt). We mask the second sentence and take something like a cosine similarity / set difference between the two vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import BertForNextSentencePrediction\n",
    "model = BertForNextSentencePrediction.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\n",
    "next_sentence = \"The sky is blue due to the shorter wavelength of blue light.\"\n",
    "\n",
    "encoding = tokenizer(prompt, next_sentence, return_tensors=\"pt\")\n",
    "outputs = model(**encoding, labels=torch.LongTensor([1]))\n",
    "\n",
    "logits = outputs.logits\n",
    "print(logits[0, 0], logits[0, 1])  # next sentence was random\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "- What current approaches to creativity evaluation have been implemented so far?\n",
    "\n",
    "### Related Work\n",
    "- Current methods/similar work with POS tags, proof that POS tags are (not) enough to capture creativity\n",
    "\n",
    "\n",
    "- Putting some ideas of the notebook into the thesis\n",
    "- Write down some methodology or datasets \n",
    "- Write down some datasets for the experiment\n",
    "- Draw some comparisons between the suggested replacements in the masked model - maybe mean, cosine similarity, etc.\n",
    "- Maybe use stuff like word2vec to show differences between the words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative implementation for semantic distance measure which abuses masked language models for context-aware words\n",
    "\n",
    "Initialize models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find average similarity of suggested tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report took ~10.459s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Prediction(word='lazy', original_tag='ADJ', suggestions=('big', 'little', 'small', 'dead', 'other', 'startled', 'running', 'hot', 'first', 'old'), probs=(7.870898246765137, 7.52618408203125, 7.143657684326172, 6.71775484085083, 6.5941901206970215, 6.483197212219238, 6.4429545402526855, 6.321310043334961, 6.017706871032715, 5.91456937789917)),\n",
       " Prediction(word='dog.The', original_tag='NOUN', suggestions=(',', 'and', '-', 'but', 'little', 'yet', '.', 'old', 'dog.Thely', 'or'), probs=(11.489553451538086, 9.219938278198242, 8.782470703125, 7.88928747177124, 6.381136417388916, 5.530984878540039, 5.135603904724121, 4.5684051513671875, 4.189810752868652, 4.081477165222168)),\n",
       " Prediction(word='quick', original_tag='ADJ', suggestions=('little', 'big', 'lazy', 'fat', 'small', 'tiny', 'large', 'old', 'great', 'giant'), probs=(11.207507133483887, 9.767521858215332, 9.288948059082031, 9.148782730102539, 8.490447044372559, 8.138623237609863, 7.948504447937012, 7.936158180236816, 7.470635890960693, 7.371654987335205)),\n",
       " Prediction(word='brown', original_tag='NOUN', suggestions=('little', 'thinking', '-', 'brownening', 'browning', 'old', 'talking', 'brownen', 'browny', 'running'), probs=(7.503300189971924, 7.210907936096191, 7.09079122543335, 7.027312755584717, 7.013429641723633, 6.600052356719971, 6.553625106811523, 6.53891134262085, 6.310474395751953, 6.2630934715271)),\n",
       " Prediction(word='fox', original_tag='NOUN', suggestions=('dog', 'that', 'cat', 'foxie', 'who', 'bear', 'puppy', 'one', 'wolf', 'man'), probs=(9.786643028259277, 8.66386890411377, 8.140987396240234, 8.054875373840332, 7.214043140411377, 7.158978462219238, 6.885220050811768, 6.729588031768799, 6.71671199798584, 6.463825225830078)),\n",
       " Prediction(word='jumped', original_tag='VERB', suggestions=('watches', '.', 'watching', 'takes', 'watched', 'took', ',', 'ran', 'runs', 'wins'), probs=(8.1289701461792, 7.9970831871032715, 7.929543495178223, 7.905054092407227, 7.9044952392578125, 7.726096153259277, 7.61280632019043, 7.201698303222656, 7.086895942687988, 6.8299241065979)),\n",
       " Prediction(word='lazy', original_tag='ADJ', suggestions=('big', 'little', 'small', 'dead', 'other', 'startled', 'running', 'hot', 'first', 'old'), probs=(7.870898246765137, 7.52618408203125, 7.143657684326172, 6.71775484085083, 6.5941901206970215, 6.483197212219238, 6.4429545402526855, 6.321310043334961, 6.017706871032715, 5.91456937789917)),\n",
       " Prediction(word='dog.The', original_tag='NOUN', suggestions=(',', 'and', '-', 'but', 'little', 'yet', '.', 'old', 'dog.Thely', 'or'), probs=(11.489553451538086, 9.219938278198242, 8.782470703125, 7.88928747177124, 6.381136417388916, 5.530984878540039, 5.135603904724121, 4.5684051513671875, 4.189810752868652, 4.081477165222168)),\n",
       " Prediction(word='quick', original_tag='ADJ', suggestions=('little', 'big', 'lazy', 'fat', 'small', 'tiny', 'large', 'old', 'great', 'giant'), probs=(11.207507133483887, 9.767521858215332, 9.288948059082031, 9.148782730102539, 8.490447044372559, 8.138623237609863, 7.948504447937012, 7.936158180236816, 7.470635890960693, 7.371654987335205)),\n",
       " Prediction(word='brown', original_tag='NOUN', suggestions=('little', 'thinking', '-', 'brownening', 'browning', 'old', 'talking', 'brownen', 'browny', 'running'), probs=(7.503300189971924, 7.210907936096191, 7.09079122543335, 7.027312755584717, 7.013429641723633, 6.600052356719971, 6.553625106811523, 6.53891134262085, 6.310474395751953, 6.2630934715271)),\n",
       " Prediction(word='fox', original_tag='NOUN', suggestions=('dog', 'that', 'cat', 'foxie', 'who', 'bear', 'puppy', 'one', 'wolf', 'man'), probs=(9.786643028259277, 8.66386890411377, 8.140987396240234, 8.054875373840332, 7.214043140411377, 7.158978462219238, 6.885220050811768, 6.729588031768799, 6.71671199798584, 6.463825225830078)),\n",
       " Prediction(word='jumped', original_tag='VERB', suggestions=('watches', '.', 'watching', 'takes', 'watched', 'took', ',', 'ran', 'runs', 'wins'), probs=(8.1289701461792, 7.9970831871032715, 7.929543495178223, 7.905054092407227, 7.9044952392578125, 7.726096153259277, 7.61280632019043, 7.201698303222656, 7.086895942687988, 6.8299241065979)),\n",
       " Prediction(word='lazy', original_tag='ADJ', suggestions=('big', 'little', 'small', 'dead', 'other', 'startled', 'running', 'hot', 'first', 'old'), probs=(7.870898246765137, 7.52618408203125, 7.143657684326172, 6.71775484085083, 6.5941901206970215, 6.483197212219238, 6.4429545402526855, 6.321310043334961, 6.017706871032715, 5.91456937789917)),\n",
       " Prediction(word='dog.The', original_tag='NOUN', suggestions=(',', 'and', '-', 'but', 'little', 'yet', '.', 'old', 'dog.Thely', 'or'), probs=(11.489553451538086, 9.219938278198242, 8.782470703125, 7.88928747177124, 6.381136417388916, 5.530984878540039, 5.135603904724121, 4.5684051513671875, 4.189810752868652, 4.081477165222168)),\n",
       " Prediction(word='quick', original_tag='ADJ', suggestions=('little', 'big', 'lazy', 'fat', 'small', 'tiny', 'large', 'old', 'great', 'giant'), probs=(11.207507133483887, 9.767521858215332, 9.288948059082031, 9.148782730102539, 8.490447044372559, 8.138623237609863, 7.948504447937012, 7.936158180236816, 7.470635890960693, 7.371654987335205)),\n",
       " Prediction(word='brown', original_tag='NOUN', suggestions=('little', 'thinking', '-', 'brownening', 'browning', 'old', 'talking', 'brownen', 'browny', 'running'), probs=(7.503300189971924, 7.210907936096191, 7.09079122543335, 7.027312755584717, 7.013429641723633, 6.600052356719971, 6.553625106811523, 6.53891134262085, 6.310474395751953, 6.2630934715271)),\n",
       " Prediction(word='fox', original_tag='NOUN', suggestions=('dog', 'that', 'cat', 'foxie', 'who', 'bear', 'puppy', 'one', 'wolf', 'man'), probs=(9.786643028259277, 8.66386890411377, 8.140987396240234, 8.054875373840332, 7.214043140411377, 7.158978462219238, 6.885220050811768, 6.729588031768799, 6.71671199798584, 6.463825225830078)),\n",
       " Prediction(word='jumped', original_tag='VERB', suggestions=('watches', '.', 'watching', 'takes', 'watched', 'took', ',', 'ran', 'runs', 'wins'), probs=(8.1289701461792, 7.9970831871032715, 7.929543495178223, 7.905054092407227, 7.9044952392578125, 7.726096153259277, 7.61280632019043, 7.201698303222656, 7.086895942687988, 6.8299241065979)),\n",
       " Prediction(word='lazy', original_tag='ADJ', suggestions=('big', 'little', 'small', 'dead', 'other', 'startled', 'running', 'hot', 'first', 'old'), probs=(7.870898246765137, 7.52618408203125, 7.143657684326172, 6.71775484085083, 6.5941901206970215, 6.483197212219238, 6.4429545402526855, 6.321310043334961, 6.017706871032715, 5.91456937789917)),\n",
       " Prediction(word='dog.The', original_tag='NOUN', suggestions=(',', 'and', '-', 'but', 'little', 'yet', '.', 'old', 'dog.Thely', 'or'), probs=(11.489553451538086, 9.219938278198242, 8.782470703125, 7.88928747177124, 6.381136417388916, 5.530984878540039, 5.135603904724121, 4.5684051513671875, 4.189810752868652, 4.081477165222168)),\n",
       " Prediction(word='quick', original_tag='ADJ', suggestions=('little', 'big', 'lazy', 'fat', 'small', 'tiny', 'large', 'old', 'great', 'giant'), probs=(11.207507133483887, 9.767521858215332, 9.288948059082031, 9.148782730102539, 8.490447044372559, 8.138623237609863, 7.948504447937012, 7.936158180236816, 7.470635890960693, 7.371654987335205)),\n",
       " Prediction(word='brown', original_tag='NOUN', suggestions=('little', 'thinking', '-', 'brownening', 'browning', 'old', 'talking', 'brownen', 'browny', 'running'), probs=(7.503300189971924, 7.210907936096191, 7.09079122543335, 7.027312755584717, 7.013429641723633, 6.600052356719971, 6.553625106811523, 6.53891134262085, 6.310474395751953, 6.2630934715271)),\n",
       " Prediction(word='fox', original_tag='NOUN', suggestions=('dog', 'that', 'cat', 'foxie', 'who', 'bear', 'puppy', 'one', 'wolf', 'man'), probs=(9.786643028259277, 8.66386890411377, 8.140987396240234, 8.054875373840332, 7.214043140411377, 7.158978462219238, 6.885220050811768, 6.729588031768799, 6.71671199798584, 6.463825225830078)),\n",
       " Prediction(word='jumped', original_tag='VERB', suggestions=('watches', '.', 'watching', 'takes', 'watched', 'took', ',', 'ran', 'runs', 'wins'), probs=(8.1289701461792, 7.9970831871032715, 7.929543495178223, 7.905054092407227, 7.9044952392578125, 7.726096153259277, 7.61280632019043, 7.201698303222656, 7.086895942687988, 6.8299241065979)),\n",
       " Prediction(word='lazy', original_tag='ADJ', suggestions=('big', 'little', 'small', 'dead', 'other', 'startled', 'running', 'hot', 'first', 'old'), probs=(7.870898246765137, 7.52618408203125, 7.143657684326172, 6.71775484085083, 6.5941901206970215, 6.483197212219238, 6.4429545402526855, 6.321310043334961, 6.017706871032715, 5.91456937789917)),\n",
       " Prediction(word='dog.The', original_tag='NOUN', suggestions=(',', 'and', '-', 'but', 'little', 'yet', '.', 'old', 'dog.Thely', 'or'), probs=(11.489553451538086, 9.219938278198242, 8.782470703125, 7.88928747177124, 6.381136417388916, 5.530984878540039, 5.135603904724121, 4.5684051513671875, 4.189810752868652, 4.081477165222168)),\n",
       " Prediction(word='quick', original_tag='ADJ', suggestions=('little', 'big', 'lazy', 'fat', 'small', 'tiny', 'large', 'old', 'great', 'giant'), probs=(11.207507133483887, 9.767521858215332, 9.288948059082031, 9.148782730102539, 8.490447044372559, 8.138623237609863, 7.948504447937012, 7.936158180236816, 7.470635890960693, 7.371654987335205)),\n",
       " Prediction(word='brown', original_tag='NOUN', suggestions=('little', 'thinking', '-', 'brownening', 'browning', 'old', 'talking', 'brownen', 'browny', 'running'), probs=(7.503300189971924, 7.210907936096191, 7.09079122543335, 7.027312755584717, 7.013429641723633, 6.600052356719971, 6.553625106811523, 6.53891134262085, 6.310474395751953, 6.2630934715271)),\n",
       " Prediction(word='fox', original_tag='NOUN', suggestions=('dog', 'that', 'cat', 'foxie', 'who', 'bear', 'puppy', 'one', 'wolf', 'man'), probs=(9.786643028259277, 8.66386890411377, 8.140987396240234, 8.054875373840332, 7.214043140411377, 7.158978462219238, 6.885220050811768, 6.729588031768799, 6.71671199798584, 6.463825225830078)),\n",
       " Prediction(word='jumped', original_tag='VERB', suggestions=('takes', 'watching', '.', 'took', 'watches', 'watched', ',', 'runs', 'ran', 'stood'), probs=(7.904865264892578, 7.680716514587402, 7.528406143188477, 7.483312606811523, 7.45817232131958, 7.209831237792969, 7.166139602661133, 7.129114151000977, 7.04945707321167, 6.542450904846191))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from string import punctuation\n",
    "from madhatter.utils import stopwords\n",
    "from madhatter.benchmark import CreativityBenchmark\n",
    "from madhatter.models import sliding_window_preds_tagged, default_word2vec, default_model\n",
    "\n",
    "text = \"The quick brown fox jumped over the lazy dog.The quick brown fox jumped over the lazy dog.The quick brown fox jumped over the lazy dog.The quick brown fox jumped over the lazy dog.The quick brown fox jumped over the lazy dog.The quick brown fox jumped over the lazy dog.\"\n",
    "\n",
    "bench = CreativityBenchmark(text)\n",
    "\n",
    "bench.report(include_llm=True)\n",
    "stopwords = stopwords.union(set(punctuation))\n",
    "# model, tokenizer = default_model()\n",
    "# \n",
    "\n",
    "# preds = sliding_window_preds_tagged(bench.tagged_words(\n",
    "# )[:1000], model, tokenizer, return_tokens=True, k=5, tags_of_interest=bench.tags_of_interest, stopwords=stopwords)\n",
    "# preds\n",
    "\n",
    "# be warned that running this without any stopwords, you double and triple the time required\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import madhatter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "dc2ee436cbdf7adb3462c2431d641fae6796d2f28e77e1bb10007c2aae13c49d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
