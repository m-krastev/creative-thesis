\chapter{Related Work}
\label{chap:background}
The field of creativity research has been studied for decades, and there are many different approaches to the problem. In this section, we will discuss the different approaches to creativity research, and how they relate to our work. We will also discuss the different approaches to creativity in the context of natural language processing, and how they relate to our work.
% Include background, what previous authors have done in a mostly neutral style. Optimally expecting ~1000-2000 words and 30-40 references mentioned.

\section{Challenge Landscape}
\label{sec:challenge_landscape}
Most of the work going on in our minds as we read a given text is unconscious. We automatically parse characters and tokenize the text into words, sentences, paragraphs, and so on. At the same time, we apply tagging to understand actors (subjects and objects), actions (verbs), setting and situation (adverb), and also try to understand the sense a given word is used in, and then we transform the connotations or the sense of words inside our own little mind representation of the words. We also apply sentiment analysis to understand the emotional state of the text (or speech), and we apply phonetic analysis to understand the pronunciation of the words, as we read them out in our minds, e.g. when reading a book. Note that we have constrained the example of autonomous processes to reading, although these subconscious processes happen regardless of whether we read, write, or speak. Fact of the matter is, language is inherently a complex social construct that takes years to learn, even more to master, and more than a lifetime to perfect. Because it is so expressive and hard to grasp, many rules have been invented and applied to constrain or clearly define the boundaries of the language (e.g. English), so that, within a given language speaker group (e.g. the space of English speakers), the biggest common denominator of people may understand what is being said by the others. The rules may additionally have their own rules and exceptions to the rules. 


Therefore, there are multiple approaches being applied to natural language processing tasks. The more classical one, and the one that has been applied for the larger part of the developments in the field, has been the rule-based method. The approach seeks to use clearly defined rules to parse and understand the linguistic features of the given text. It is a strong approach, as language has been strongly studied for centuries and the aforementioned rules and constraints have already been applied to it. For languages with few changing features or slowly changing features, it performs acceptably well on most \acrfull{nlp} tasks, and not far off from human speakers. \note{expand with 1-2 paragraphs and \textbf{references pls}}

Another approach or a subclass of the rule-based approach, is the \textbf{statistical method} \mk{this statement is not bs, right?}. The statistical method seeks to find patterns inside language as a whole. For example, the word ``wind'' may appear both as a noun and a verb -- that is, the meaning of the word may be ambiguous -- but the noun form is much more prevalent. Therefore, in tasks such as part of speech (PoS) tagging, some algorithms prefer to use the most common type of PoS class a given word occurs as, in a sufficiently large corpus of text. We do come back to the PoS tagging example later on. Alternative example of the statistical method being applied would be translation. Previous approaches to machine translation (MTL) included learning frequency of words and phrases occurring together (and how those map to counterparts in the language being translated to). For example:
\begin{quotation}
    Finding Nemo is like finding fish in a school of fish.
\end{quotation}
A naive approach to the translation of this sentence would consider school as its most common (noun) definition - that of place of learning for \textit{humans} and translated the word literally. A more sophisticated approach to translation, applying not just simple rules to translating, would consider the whole phrase ``school of fish'', and would have understood that it refers to a countable form of the word fish (relating to a large number of fish), and therefore translated the phrase as a whole to the target language.

The most current approach to many of the challenging NLP tasks (text summarization, machine translation, speech recognition, etc.) is a machine-learning based one. The motivation is multifold: firstly, most of the work that goes on in our minds as we read a given text is unconscious and automatic, just as we do not have to consciously intend to breathe in order to breathe. In much the same way, we rarely consciously make an effort to understand every part of the text, and many of the details behind understanding language are vague and ambiguous (consider how someone would respond if they are asked to explain their thought process behind parsing a given text). Secondly, more in line with the topic of our research, we do not have a good (objective) way to measure the quality of the work. It is subjective and difficult to measure. Not to mention, behaviour -- and consequently, consciousness -- is something that arises from environment, upbringing, culture, and so on. Yet, value for quality is something that multiple individuals can share -- many people can have a sense of appreciation for a novel they read or a speech they heard (of course, usually for slightly different reasons and perceptions) -- but there tend to be common elements that people widely enjoy seeing and experiencing.

The intuition of machine learning and deep learning is that you can try to replicate the unconscious logical circuitry going on behind the scenes without having a very solid grasp of the exact logic behind it. Therefore, in fields with few or changing rules, such as linguistics, music, and image processing, the machine learning approach tends to find large success, and has even been shown recently to be able to perform very similarly to humans \citep{bubeck2023sparks}. All in all, we cannot ignore the potential for machine learning to be applied to the field of creativity, and we explore this potential in our work.

We, therefore, go into detail on some of the methods we utilize in the project, and the various approaches that have been taken in the past.

\subsection{Part of Speech Tagging}
As we have established, a word may play a different role depending on its position in the sentence, both absolute and relative to the other words. Words that denote objects or persons, we generally call \textbf{nouns}, while words that denote (usually active) actions, we call \textbf{verbs}, and so on. 
Part-of-speech tagging as a task poses the challenge of assigning to the sequence of tokens (we use ``tokens'' and ``words'' interchangeably) $x_1, x_2, \dots, x_n$, a sequence of tags $y_1, y_2, \dots, y_n$, where each token $x_i$ has a corresponding tag $y_i$, making tagging a task of disambiguation, that is, removing ambiguities in text.

The phenomenon of ambiguity is hardly exclusive to the English language, although it is a key problem that we would need to take head-on, as many of the tasks we do want to tackle, rely on POS tagging, in order to avoid needless computations (and in consideration of the limited computing resources for both end-users and ourselves, as we may do larger-scale experiments). Human performance on POS-tagging has been shown to be around 97\% accuracy \citep{manning2011part} for English texts. We will use this as a base of comparison with automatic algorithms.

\begin{table}[htbp]
    \centering
    \begin{tabular}{p{0.45\textwidth}cc}
        \toprule
        \textbf{Types:} & \textbf{WSJ} & \textbf{Brown} \\
        \midrule
        \quad Unambiguous (1 tag) &   44,432 (86\%) & 45,799 (85\%) \\
        \quad Ambiguous (2+ tags) & 7,025 (14\%)  & 8,050 (15\%) \\
        \midrule
        \textbf{Tokens:} & & \\
        \midrule
        \quad Unambiguous (1 tag) & 577,421 (45\%) &  384,349 (33\%) \\
        \quad Ambiguous (2+ tags) & 711,780 (55\%) & 786,646 (67\%) \\
        \bottomrule
    \end{tabular}
    \caption{Tag ambiguity in the Brown and WSJ corpora (Treebank-3 45-tag tagset). \\ Adapted from \cite{Jurafsky:2009:SLP:1214993}}.
    \label{tab:tokens_textbook}
\end{table}
As seen in Table \ref{tab:tokens_textbook}, many of the words in the tag-annotated corpora are, in fact, unambiguous. That is, the given word appears only as one single \textit{type} of part of speech. What we may miss, however, is the fact that the actual \textit{tokens}, the words, are mostly ambiguous (55\% on WSJ and 67\% on Brown). For example, \textit{elephant} can only appear as a noun, but the word \textit{back} can appear as either a verb, an adjective, a noun, or an adverb. And \textit{back} is a much more common word than, say, \textit{elephant}. Given this example, we can then proceed to potential solutions.

Generally, the issue is hardly new in the field of text mining, and solutions have been attempted since long ago in the past. The Brown corpus by \cite{francis1979brown} is a manually annotated for parts of speech corpus of American English texts from variety of genres, and has been widely used as a benchmark for algorithms for POS-tagging -- at least ones focusing on the English language. 

One baseline metric is the \textbf{most frequent class} classifier. This method assigns to each word the most frequent tag it appears as in some training corpus. The metric has been shown to be 92\% effective \citep{Jurafsky:2009:SLP:1214993} in correctly classifying the tags of a given text, which is just 5\% below human accuracy, as described above. However, we can do better.

Hidden Markov Models are among the more successful ones, as demonstrated by \cite{schutze1994part,thede1999second,nigam1999maximumentropy}. The idea stems from the fact that the probability of a word $x_i$ appearing in a given position in a sentence, is dependent on the word that came before it. This is a Markov assumption, as it assumes that the probability of a word appearing in a given position is independent of the words that come after it. The given assumption is a strong one, and it is not always true. However, it is a good starting point, and it has been shown to be effective in many cases. The 

While the HMM is a useful and powerful model, it turns out that HMMs need a number of augmentations to achieve high accuracy, as exemplified by authors such as \cite{goldberg2008can}. \cite{10.3115/974147.974178postaggingmarkov} demonstrate between 96 and 99\% accuracy for HMMs, while \cite{thede1999second} demonstrate an accuracy between 96-98\%, and between 88 and 93\% accuracy on various datasets by \cite{goldberg2008can}. For example, in POS tagging as in other tasks, we often run into unknown words: proper names and acronyms are created very often, and even new common nouns and verbs enter the language at a surprising rate -- the Oxford English Dictionary has added 700 new words to its definitions just in its most recent quarter-yearly update in March \citep{oed2023englishupdates}. 

Conditional Random Fields (CRFs) have been proposed to deal with the following motivations: the adding of arbitrary features, e.g. based on capitalization or morphology (words starting with capital letters are likely to be proper nouns, words ending with -ed tend to be past tense (VBD or VBN), etc.); knowing the previous or following words (if the previous word is ``the'', the current one is not likely to be a verb).

State-of-the-art \acrshort{pos} taggers use neural networks behind the hood, being either bidirectional RNNs or Transformers like BERT. 

\subsection{Word Sense Disambiguation}

\note{both of these can be included as potential metrics}
\subsection{Sentiment Analysis}

\subsection{Named Entity Recognition}

% Maybe not discuss this in general
\subsection{Phonetic Analysis}

\subsection{Natural Language Generation}
\begin{itemize}
    \item top KP sampling
    \item challenges 
\end{itemize}


\section{Creative Measures}
The field of creativity has been broadly studied, initially by psychologists, and more recently by computer scientists. 
Intuitively, the nature of creativity is a subjective matter, and therefore, it is difficult to define. However, there are some commonalities that can be found in creative works. For example, creativity is often associated with novelty, and is often associated with the ability to generate new ideas. \cite{franceschelli_deepcreativity_2022}, for example, determine the three factors of creativity as value, novelty, and surprise, and then explore machine learning approaches to measuring creativity. We agree with them, but decide not to limit ourselves to just these three. However, their contributions provide insights we can use in our work. \mk{did we use it - if we did not, delete this sentence} 
\begin{itemize}
    \item Burstiness of verbs and derived nouns: Patterns of language are sometimes `bursty' \cite{pierrehumbert_burstiness_2012}. This paper presents an analysis of text patterns for domain X. measures include XYZ...
    \item 
\end{itemize}

\section{Tools}
