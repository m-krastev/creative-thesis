\chapter{Related Work}
\label{chap:background}
The field of creativity research has been studied for decades, and there are many different approaches to the problem. In this section, we will discuss the different approaches to creativity research, and how they relate to our work. We will also discuss the different approaches to creativity in the context of natural language processing, and how they relate to our work.
% Include background, what previous authors have done in a mostly neutral style. Optimally expecting ~1000-2000 words and 30-40 references mentioned.

\section{Challenge Landscape}
\label{sec:challenge_landscape}
Most of the work going on in our minds as we read a given text is unconscious. We automatically parse characters and tokenize the text into words, sentences, paragraphs, and so on. At the same time, we apply tagging to understand actors (subjects and objects), actions (verbs), setting and situation (adverb), and also try to understand the sense a given word is used in, and then we transform the connotations or the sense of words inside our own little mind representation of the words. We also apply sentiment analysis to understand the emotional state of the text (or speech), and we apply phonetic analysis to understand the pronunciation of the words, as we read them out in our minds, e.g. when reading a book. Note that we have constrained the example of autonomous processes to reading, although these subconscious processes happen regardless of whether we read, write, or speak. Fact of the matter is, language is inherently a complex social construct that takes years to learn, even more to master, and more than a lifetime to perfect. Because it is so expressive and hard to grasp, many rules have been invented and applied to constrain or clearly define the boundaries of the language (e.g. English), so that, within a given language speaker group (e.g. the space of English speakers), the biggest common denominator of people may understand what is being said by the others. The rules may additionally have their own rules and exceptions to the rules. 


Therefore, there are multiple approaches being applied to natural language processing tasks. The more classical one, and the one that has been applied for the larger part of the developments in the field, has been the rule-based method. The approach seeks to use clearly defined rules to parse and understand the linguistic features of the given text. It is a strong approach, as language has been strongly studied for centuries and the aforementioned rules and constraints have already been applied to it. For languages with few changing features or slowly changing features, it performs acceptably well on most \acrfull{nlp} tasks, and not far off from human speakers. \note{expand with 1-2 paragraphs and \textbf{references pls}}

Another approach or a subclass of the rule-based approach, is the \textbf{statistical method} \mk{this statement is not bs, right?}. The statistical method seeks to find patterns inside language as a whole. For example, the word ``wind'' may appear both as a noun and a verb -- that is, the meaning of the word may be ambiguous -- but the noun form is much more prevalent. Therefore, in tasks such as part of speech (PoS) tagging, some algorithms prefer to use the most common type of PoS class a given word occurs as, in a sufficiently large corpus of text. We do come back to the PoS tagging example later on. Alternative example of the statistical method being applied would be translation. Previous approaches to machine translation (MTL) included learning frequency of words and phrases occurring together (and how those map to counterparts in the language being translated to). For example:
\begin{quotation}
    Finding Nemo is like finding fish in a school of fish.
\end{quotation}
A naive approach to the translation of this sentence would consider school as its most common (noun) definition - that of place of learning for \textit{humans} and translated the word literally. A more sophisticated approach to translation, applying not just simple rules to translating, would consider the whole phrase ``school of fish'', and would have understood that it refers to a countable form of the word fish (relating to a large number of fish), and therefore translated the phrase as a whole to the target language.

The most current approach to many of the challenging NLP tasks (text summarization, machine translation, speech recognition, etc.) is a machine-learning based one. The motivation is multifold: firstly, most of the work that goes on in our minds as we read a given text is unconscious and automatic, just as we do not have to consciously intend to breathe in order to breathe. In much the same way, we rarely consciously make an effort to understand every part of the text, and many of the details behind understanding language are vague and ambiguous (consider how someone would respond if they are asked to explain their thought process behind parsing a given text). Secondly, more in line with the topic of our research, we do not have a good (objective) way to measure the quality of the work. It is subjective and difficult to measure. Not to mention, behaviour -- and consequently, consciousness -- is something that arises from environment, upbringing, culture, and so on. Yet, value for quality is something that multiple individuals can share -- many people can have a sense of appreciation for a novel they read or a speech they heard (of course, usually for slightly different reasons and perceptions) -- but there tend to be common elements that people widely enjoy seeing and experiencing.

The intuition of machine learning and deep learning is that you can try to replicate the unconscious logical circuitry going on behind the scenes without having a very solid grasp of the exact logic behind it. Therefore, in fields with few or changing rules, such as linguistics, music, and image processing, the machine learning approach tends to find large success, and has even been shown recently to be able to perform very similarly to humans \citep{bubeck2023sparks}. All in all, we cannot ignore the potential for machine learning to be applied to the field of creativity, and we explore this potential in our work.

We, therefore, go into detail on some of the methods we utilize in the project, and the various approaches that have been taken in the past.

\subsection{Part of Speech Tagging}
As we have established, a word may play a different role depending on its position in the sentence, both absolute and relative to the other words. Words that denote objects or persons, we generally call \textbf{nouns}, while words that denote (usually active) actions, we call \textbf{verbs}, and so on. 
Part-of-speech tagging as a task poses the challenge of assigning to the sequence of tokens (we use ``tokens'' and ``words'' interchangeably) $x_1, x_2, \dots, x_n$, a sequence of tags $y_1, y_2, \dots, y_n$, where each token $x_i$ has a corresponding tag $y_i$, making tagging a task of disambiguation, that is, removing ambiguities in text.

The phenomenon of ambiguity is hardly exclusive to the English language, although it is a key problem that we would need to take head-on, as many of the tasks we do want to tackle, rely on POS tagging, in order to avoid needless computations (and in consideration of the limited computing resources for both end-users and ourselves, as we may do larger-scale experiments). Human performance on POS-tagging has been shown to be around 97\% accuracy \citep{manning2011part} for English texts. We will use this as a base of comparison with automatic algorithms.

\begin{table}[htbp]
    \centering
    \begin{tabular}{p{0.45\textwidth}cc}
        \toprule
        \textbf{Types:} & \textbf{WSJ} & \textbf{Brown} \\
        \midrule
        \quad Unambiguous (1 tag) &   44,432 (86\%) & 45,799 (85\%) \\
        \quad Ambiguous (2+ tags) & 7,025 (14\%)  & 8,050 (15\%) \\
        \midrule
        \textbf{Tokens:} & & \\
        \midrule
        \quad Unambiguous (1 tag) & 577,421 (45\%) &  384,349 (33\%) \\
        \quad Ambiguous (2+ tags) & 711,780 (55\%) & 786,646 (67\%) \\
        \bottomrule
    \end{tabular}
    \caption{Tag ambiguity in the Brown and WSJ corpora (Treebank-3 45-tag tagset). \\ Adapted from \cite{Jurafsky:2009:SLP:1214993}}.
    \label{tab:tokens_textbook}
\end{table}
As seen in Table \ref{tab:tokens_textbook}, many of the words in the tag-annotated corpora are, in fact, unambiguous. That is, the given word appears only as one single \textit{type} of part of speech. What we may miss, however, is the fact that the actual \textit{tokens}, the words, are mostly ambiguous (55\% on WSJ and 67\% on Brown). For example, \textit{elephant} can only appear as a noun, but the word \textit{back} can appear as either a verb, an adjective, a noun, or an adverb. And \textit{back} is a much more common word than, say, \textit{elephant}. Given this example, we can then proceed to potential solutions.

Generally, the issue is hardly new in the field of text mining, and solutions have been attempted since long ago in the past. The Brown corpus by \cite{francis1979brown} is a manually annotated for parts of speech corpus of American English texts from variety of genres, and has been widely used as a benchmark for algorithms for POS-tagging -- at least ones focusing on the English language. 

One baseline metric is the \textbf{most frequent class} classifier. This method assigns to each word the most frequent tag it appears as in some training corpus. The metric has been shown to be 92\% effective \citep{Jurafsky:2009:SLP:1214993} in correctly classifying the tags of a given text, which is just 5\% below human accuracy, as described above. However, we can do better.

Hidden Markov Models are among the more successful ones, as demonstrated by \cite{schutze1994part,thede1999second,nigam1999maximumentropy}. The idea stems from the fact that the probability of a word $x_i$ appearing in a given position in a sentence, is dependent on the word that came before it. This is a Markov assumption, as it assumes that the probability of a word appearing in a given position is independent of the words that come after it. The given assumption is a strong one, and it is not always true. However, it is a good starting point, and it has been shown to be effective in many cases. The 

While the HMM is a useful and powerful model, it turns out that HMMs need a number of augmentations to achieve high accuracy, as exemplified by authors such as \cite{goldberg2008can}. \cite{10.3115/974147.974178postaggingmarkov} demonstrate between 96 and 99\% accuracy for HMMs, while \cite{thede1999second} demonstrate an accuracy between 96-98\%, and between 88 and 93\% accuracy on various datasets by \cite{goldberg2008can}. For example, in POS tagging as in other tasks, we often run into unknown words: proper names and acronyms are created very often, and even new common nouns and verbs enter the language at a surprising rate -- the Oxford English Dictionary has added 700 new words to its definitions just in its most recent quarter-yearly update in March \citep{oed2023englishupdates}. 

Conditional Random Fields (CRFs) have been proposed to deal with the following motivations: the adding of arbitrary features, e.g. based on capitalization or morphology (words starting with capital letters are likely to be proper nouns, words ending with -ed tend to be past tense (VBD or VBN), etc.); knowing the previous or following words (if the previous word is ``the'', the current one is not likely to be a verb).

State-of-the-art \acrshort{pos} taggers use neural networks behind the hood, being either bidirectional RNNs or Transformers like BERT. Although for 99.9\% of applications we do not need the computational load of those in order to achieve the highest possible accuracy.

\subsection{Named Entity Recognition}
Named Entity Recognition (NER) is a task that seeks to identify and classify named entities in text into pre-defined categories. Named entities are words or phrases that are of specific types, such as names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc. The task of NER is to identify these entities, and classify them into their respective categories. Phrases such as ``The University of Aberdeen'', British pound, or the UN are all examples of \textbf{named entities}.

Similar in nature to the task of POS tagging, named entity recognition is a relatively well known task with demonstrably high accuracy in terms of success. The task is also relatively well defined, and there are a number of datasets that can be used to train and test the algorithms.

Parts of speech and named entities both are useful indicators for structure and help understand the context of a given sentence better. Knowing whether a given word is a noun or a verb not only aids in understanding whether the word is an actor or an action, but it gives hints about the structure of a sentence (for example, nouns are commonly preceded by determiners and adjectives), and the syntactic parts of the sentence (a noun is likely to be the subject of a sentence, while a verb is likely to be the predicate accompanying the subject). In much the same vein, named entity recognition is important in understanding the context of a sentence -- disambiguating between \textit{Aberdeen}, the city, the university, and the general area, is key to many natural language processing tasks such as question answering, text summarization, and information extraction.

Unlike POS tagging, however, NER has more to do with identifying which spans of text contain named entities. The standard approach to labeling for a span-recognition problem such as this one is BIO tagging \cite{ramshaw-marcus-1995-text-ner}. The method has been shown to treat NER like a word-by-word sequence labeling task, via tags that capture both boundary and named entity type. A sequence labeler (HMM, CRF, RNN, Transformer, etc.) is trained to label each token in a text with tags that indicate the presence (or absence) of particular kinds of named entities. Usually, however, many commercial implementations of NER algorithms use a rule-based approach to the problem, as it is more efficient at scale \citep{li-etal-2011-systemt-ner}. 


\subsection{Word Sense Disambiguation}

The study of word sense disambiguation refers to the task of determining the sense of a word in a given context. For example, the word ``bank'' can have multiple meanings, depending on the context. In the sentence ``I went to the bank to withdraw some money'', the word ``bank'' refers to a financial institution. However, in the sentence ``I sat on the bank of the river'', the word ``bank'' refers to the edge of the river. This is a difficult task, as it requires the system to understand the context of the word, and the meaning of the word in that context.

Various approaches have been suggested, such as the Lesk algorithm \cite{10.1145/318723.318728} which tries to look for some overlap between the usages of two given words in their general context. Additionally, the Simplified Lesk algorithm has been proposed \citep{kilgarriff_framework_2000_lesk} to provide a simpler computational approach to the problem. The Simplified Lesk algorithm looks for lexical overlap between the dictionary definitions of the words. The algorithm has been shown to be effective in some cases, but not always. Other approaches have been proposed, such as the Cosine Lesk and the Extended Lesk algorithm \citep{banerjee2002extended}, which uses the overlap between the dictionary definitions of the words, as well as the overlap between the dictionary definitions of the words and the words themselves. The Lesk family of algorithms provides a useful baseline for the task of word sense disambiguation, but it is not always effective.

Relations between word senses may include synonymy, antonymy, meronymy, and sometimes taxonomic relations such as hyponymy and hypernymy. WordNet \citep{wordnet1998fellbaum,wordnet_princeton} is a lexical database of English, which provides a set of relations between words. As a lexical database, it not only acts as a dictionary containing the various senses of the possible words, but it also models relations between the words, such as the ones described above. Furthermore, many languages have implemented their own WordNets, such as the Romanian WordNet \citep{tufis2004wordnet}, and the Bulgarian WordNet \citep{dimitrova-etal-2014-coping}.

Supervised approaches make use of a corpus of sentences in which individual words (lexical sample task) or all words (all-words task) are hand-labeled with senses from a resource like WordNet. Currently, SemCor \citep{semcor} is the largest known corpus with WordNet-labeled senses, in English and in general, and is actively used in testing the accuracy of approaches to word sense disambiguation. The standard supervised algorithm for WSD is 1-nearest neighbours with contextual embeddings \citep{peters-etal-2018-deep_wsd,melamud2016context2vec_wsd}.

An important baseline for WSD is the most frequent sense. In terms of computational efficiency, it is the fastest. In terms of efficacy, it performs decently, similarly to the intuition of the most frequent POS tag that was explained in the previous section. In WordNet, this equates to taking the first sense. Of course, in English at least, words can be more than one part of speech, therefore, we need to account for this when we are looking for the most frequent sense. 

There have also been some approaches to tackling the task of Word Sense Induction - attempting to learn word senses unsupervised.


\section{Numerical representations of semantic tokens}
The idea of representing words or lexical tokens as numerical vectors (or even scalars) is hardly new. 
For example the SimLex-999 dataset \citep*{hill-etal-2015-simlex} gives values on a scale from 0 to 10, like the examples below, which range from near-synonyms (vanish, disappear) to pairs that scarcely seem to have anything in common (hole, agreement):

\begin{table}[htbp]
    \centering
        \begin{tabular}{llc}
            \toprule
            word1 & word2 & score \\
            \midrule
            vanish & disappear & 9.8 \\
            hole & agreement & 1.2 \\
            \bottomrule
       \end{tabular}
    \caption{Example Simlex-999 pairs}
    \label{simlex2pairs}
\end{table}


Early work on affective meaning by \cite{osgood1957measurement} found that words varied along three important dimensions of affective meaning:
\begin{itemize}
    \item valence: the pleasantness of the stimulus
    \item arousal: the intensity of emotion provoked by the stimulus
    \item dominance: the degree of control exerted by the stimulus
\end{itemize}

\cite{osgood1957measurement} noticed that in using these 3 numbers to represent the meaning of a word, the model was representing each word as a point in a three-dimensional space, a vector whose three dimensions corresponded to the wordâ€™s rating on the three scales. This revolutionary idea that word meaning could be represented as a point in space (e.g., that part of the meaning of heartbreak can be represented as the point $[2.45,5.65,3.58]$) was the first expression of the vector semantics models that we introduce in Section \ref{sec:word2vec}.

\subsection{Concreteness}
As demonstrated by the 3 factors of valence, arousal and dominance, words may exert an effect on human understanding -- that is, words can have influence. Some of these numerical representations, therefore, can intrinsically lead to areas of psychology -- a field of study connecting psychology and language studies (linguistics) called psycholinguistics \cite{tausczik_psychological_2010}. One explored measure from this field is the concreteness aspect of a word. 

Concreteness refers to the extent to which a word refers to a tangible object or a concept that can be experienced by the senses. For example, the word ``cup'' is a concrete word, as it refers to a tangible object that can be experienced by the senses. On the other hand, the word ``time'' is an abstract word, as it refers to a concept that does not appear concrete -- not a thing one can touch or taste. 

Concreteness ratings for words have been studied since a long time ago \cite{brysbaert2014concreteness}.
\cite{brysbaert2014concreteness}, for example, carries out a study on the concreteness of words, and provides a dataset of concreteness ratings for 40,000 English words. The dataset is available online and presents an interesting topic of research. 

\subsection{Imageability}
Imageability is a measure of how easily a word can be visualized. For example, the word ``cup'' is a highly imageable word, as it is easy to visualize a cup, while ``truth'' is not a highly imageable word. As with the concreteness aspect of words, imageability is a measure that has been studied and has deep roots in the field of psycholinguistics. \cite{cortese_imageability_2004}, for example, studies the imageability of 3000 one-syllable words and how study participants react to them. Said study provides a dataset of imageability ratings for 3000 one-syllable words, and poses an interesting topic of research.

\section{Creative Measures}
The field of creativity has been broadly studied, initially by psychologists, and more recently by computer scientists, and many have pondered the nature of creativity. Is creativity a skill that can be learned, or is it an innate ability? Is creativity a skill that can be measured, or is it a subjective matter \citep{cropley2003creativitytests}?

Intuitively, the nature of creativity is a subjective matter, and therefore, it is difficult to define. Many have tried to put forward a definition of creativity \citep{runco_standard_2012}. \cite{plucker2004creativity} instates that ``researchers, when dealing with the topic of creativity, must:

\begin{enumerate}
    \item explicitly define what they mean by creativity
    \item avoid using scores of creativity measures as the sole definition of creativity (e.g., creativity is what creativity tests measure and creativity tests measure creativity, therefore we will use a score on a creativity test as our outcome variable),
    \item discuss how the definition they are using is similar to or different from other definitions, and 
    \item address the question of creativity for whom and in what context.''
\end{enumerate}

However, there are some commonalities that can be found in creative works. For example, creativity is often associated with novelty, and is often associated with the ability to generate new ideas. \cite{franceschelli_deepcreativity_2022}, for example, determine the three factors of creativity as value, novelty, and surprise, and then explore machine learning approaches to measuring creativity. We agree with them, but decide not to limit ourselves to just these three. However, their contributions provide insights we can use in our work. 

% did we use it? if we did not, delete this sentence - f u 

Attempts have been made at recognizing structure in text, such that some can recognize patterns. \cite{pierrehumbert_burstiness_2012}, for example, demonstrates that verbs and derived nouns are bursty, that is, they tend to appear in bursts. This is an example of a pattern that can be found in text, and leads to interesting insights about the structure of text.

\cite{Jordanous2012} suggests that creativity is a multi-faceted concept, and that there are many different approaches to the problem, but also puts forward a standardized procedure for evaluating creative systems. Furthermore, \cite{jordanous_modelling_2016} goes on to attempt to model creativity through identifying key components of corpora -- of text. Although more modern approaches to the field of creativity analysis have access to state-of-the-art LLMs, some of the insights of rule-based approaches can prove useful. \cite{perez_mexica_2001} provide one such rule-based approach to attempt to generate creative writing. The topic of neural text generation is also a fascinating topic of research that we feel is important in our context.

In our exploration, we investigate both statistical and structural markers of individual characteristics of text -- features that can uniquely identify a given text, along with patterns common for a given genre or author. We also investigate the effect of words on the reader, and how those effects can be used by authors to the effect of solidifying an author's style. We also hypothesize a set of novel measures that can be linked with the understanding of structure in text.  