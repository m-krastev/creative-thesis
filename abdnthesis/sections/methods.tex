\chapter{Methodology}
\label{chap:methods}

In this chapter, we explore the different datasets to be used, the methods for evaluating creativity, and the algorithms for creativity evaluation. We will furthermore discuss the strengths and limitations of the proposed methods and algorithms, including time complexity, memory constraints, and the accuracy of the results. We take an informed approach to the selection of the datasets and the methods for evaluating creativity, and discuss the reasons for our choices in detail, and support them with relevant literature as explored by other researchers in the field.

\section{Datasets}
\label{sec:datasets}
Datasets are vital for the success of any given project in the field of machine learning, and even more so when concerning linguistics. As evidenced by \cite{torralba_unbiased_2011}, the quality of the data used for training a model has a direct impact on the quality of the results. A model trained on a specific dataset, e.g. a corpus of law documents, can be expected to perform poorly on a dataset of medical documents, as the two domains are inherently different. Thus, we take particular care in planning and selecting the datasets we use. We also consider ease of use and access, as some datasets may require additional processing, others are subject to availability issues (e.g. paid datasets and corpora), and some may be too large to be used in a reasonable amount of time.
In this section, we will explore the datasets used in this project, and discuss their strengths and limitations.
\subsection{Brown Corpus}
The Brown Corpus \citep*{francis1979brown} is a widely used corpus in the field of computational linguistics, noted for the small variety of genres of literature it contains. The Corpus itself is founded on a compilation of American English literature from the year 1961. It is also small in terms of size, totalling around one million words, at least compared to modern corpora, which we also explore later on. The corpus also suffers from the issue of recency, as the works and language may be outdated for modern speakers of English.

Of interest is the fact that the corpus has been manually tagged for parts of speech, a process that tends to be error-prone. %citation good
As we will see later on, this fact has implications in terms of the supervised learning algorithms we implement for creativity evaluation. Still, we opt to utilize it primarily for prototyping purposes and drawing preliminary conclusions about the effectiveness of the implemented algorithms, rather than in-depth analysis and publication of results.

\subsection{Project Gutenberg}
Project Gutenberg\footnote[1]{\url{https://www.gutenberg.org/}} is a large collection of more than 50,000 works available in the public domain. The collection contains literature from various years and various genres and thus is suitable for training and evaluation of the developed benchmarks in the context of creativity study. 

As the Project does not offer an easy to process copy of its collection, we turn to the work of \cite{gutenberg_dataset}. The team developed a catalogue for on-demand download of the entire set of books available on the Project Gutenberg website, intended for use in the study of computational linguistics. The tool avoids the overhead of writing a web-scraper or a manual parser for the downloadable collections of Project Gutenberg books made available by third parties, as well as enables easy synchronization of newly released literature. Instead, we are only required to develop a simple pipeline for the data to be fed into the utilized systems. 

\subsection{Hierarchical Neural Story Generation}
In their work, \cite{fan_hierarchical_2018} trained a language model for text generation tasks on a dataset comprised of short stories submitted by multiple users given a particular premise (a prompt or a theme) by another user. \mk{Give an example for how one such short story would look like.} The dataset in question is technically referred to a series of posts and comments (threads) to them on the popular social media platform \textsc{Reddit}, and more tightly, the \textit{subreddit} forum \textsc{r/WritingPrompts}. The authors of the work \cite{fan_hierarchical_2018} have made the dataset available for public use, and we have used it for the purpose of evaluating the performance of our creativity benchmarks. As described by the authors on their GitHub page\footnote{\url{https://github.com/facebookresearch/fairseq/blob/main/examples/stories/README.md}}, the paper models the first 1000 tokens (words) of each story.

\mk{How do we use this dataset? You should describe the process of how we use it. }

\subsection{Discarded Datasets and Corpora}
Some datasets were considered, however, discarded due to: not being deemed applicable for the context of the application; general lack of availability of the dataset in a form that is easily accessible for our purposes; simply being infeasible to use due to the size of the dataset and the hardware constraints imposed on the project; or other reasons of similar nature.

\subsubsection*{The COCA}
The Corpus of Contemporary American English (COCA)\footnote[2]{\url{https://www.english-corpora.org/coca/}} is a large corpus of American English, containing nearly 1 billion words of text from contemporary sources. It is a collection of texts from a variety of genres, including fiction, non-fiction, and academic writing. The corpus offers a variety of tools for analysis of the data, including a concordance tool, a word frequency list, and a collocation finder. Naturally, many of those tools could be used in the field of statistical creativity analysis that we explore.

The corpus does offer limited access to the full API, as well as free samples of the data, however, the full corpus is not available for free, and the cost of acquiring it is prohibitive for the limitations set forward by the project. Nevertheless, the corpus is a valuable resource for the field of computational linguistics, and we would like to explore it further given less constraints.

\section{Word Sense Similarity}

\subsection{WordNet}\label{sec:wordnet}
WordNet\citep{wordnet1998fellbaum} is a lexical database of semantic relations between words that links words into semantic relations including synonyms, hyponyms, and meronyms. The synonyms are grouped into synsets (sets of synonyms) with short definitions and usage examples. It can thus be seen as a combination and extension of a dictionary and thesaurus \citep{enwiki:1143619785}. 

For our specific use cases, we have identified it as a valuable resource in terms of relational representation of words in semantic space. In the given context, this enables us to traverse a semantic graph for synonyms and related words for the goal of enriching potential similarity between the set of creative parts of speech (i.e., nouns, adjectives, adverbs), which we narrow down our scope to in particular. 

We can use it to look for word similarity between words. If we imagine synsets as a connected graph, usually words are similar when the distance between their synsets in terms of traversing the graph is shorter, and longer when there is no short path to reaching the other word. This is a useful property for our use case, as we can use it to determine the similarity between words in a given text, and thus, that can aid us in evaluating the creativity of the text.

% this sounds a bit weird

\subsection{Word2Vec}\label{sec:word2vec}

\citet*{mikolov_word2vec_2013} show in their work that words may be represented as dense vectors in $N$-dimensional space, and we can perform mathematical operations on them that may yield effective results in terms of word representation. What that means in our context is that we can measure similarity -- or distance -- between individual words, without knowing beforehand their part of speech of context. We can use such learned representations of words in terms of vector embeddings to determine the similarity between words in a given text.

This serves a fascinating purpose in our exploration of metrics such as surprisal that we detail further below. 

\subsection*{Measuring distance in vector representations of semantic tokens}
Intuition tells us that the dot product of vectors in $N$-dimensional space will grow when the set of vectors has similar values and decrease when the values are not similar. Thus, we can then construct the following metric for semantic similarity between vector representations of words:
$$ D(v,w) = v \times w = \sum_{i=1}^{N} v_i w_i = v_1 w_1 + v_2 w_2 + \dots + v_N w_N $$ 

The current metric, however, suffers from the problem that vectors of higher dimensions will inevitably be larger than vectors with lower dimensions. Furthermore, embedding vectors for words that occur frequently in text, tend to have high values in more dimensions, that is, they correlate with more words. The proposed solution is to normalize using the \textbf{vector length} as defined:
$$ | v| = \sqrt{\sum_{i=1}^{N}v_i^2}$$

Therefore, we obtain the following:

$$ \text{Similarity} (v, w) = \frac{v \times w}{|v| |w|} = \frac{\sum_{i=1}^{N} v_i w_i}{\sqrt{\sum_{i=1}^{N}v_i^2} \sqrt{\sum_{i=1}^{N}w_i^2}}$$

This product turns out to be the same as the cosine of the angle between two vectors:

$$ \frac{a \times b}{|a| |b|} = \cos(\theta) $$

Therefore, we will call this metric the \textbf{cosine similarity} of two words. As mentioned, the similarity grows for vectors with similar features along the same dimensions. Note the boundaries of said cosine metric: we get $-1$ for vectors which are polar opposites, $0$ for orthogonal vectors, and $1$ for equivalent vectors. Of note is the fact that such learned vector embeddings only have values in the positive ranges, thus, it is impossible to have negative values for the cosine similarity (Similarity$(a,b) \in [0,1]$).

Contrary to it, we also identify the metric of \textbf{cosine distance} between two vectors, as one minus the similarity of the vectors, or:

$$ \text{Distance}(v,w) = 1 - \text{Similarity}(v,w) $$

The cosine distance may prove useful when dealing with minimisation problems as is often the case with machine learning.

\section{Metrics}
\label{sec:metrics}

\subsection{Number of Words}
The total number of words in a given piece of text. At first glance, this metric does not impress and is, in fact, exceedingly simple. But that is fine -- we do not always need complex metrics. Sometimes, even a trivial metric as this one can inform a lot about the structure of the text. For example, the number of words in a text is directly correlated with the length of the text. This can be useful in determining the complexity of the text, as well as the time it takes to read it. In some uses, for example, comparing between books and \textit{Twitter} posts, we do not need much more information to recognize that these texts belong to entirely different genres. Such a metric is a good complement to and often used in conjunction with other metrics.

\subsection{Number of Sentences}
The number of sentences, similarly to number of words, is a trivial measure for the length of the text. However, it can be used to determine the complexity of the text. For example, a text with a large number of sentences is likely to be more complex than a text with a small number of sentences. This is because a text with a large number of sentences is likely to involve longer intellectual activity. Of course, in light of recent developments in the field of natural language generation, this metric is not particularly useful. However, due to how trivial to implement it is, it can be used in conjunction with other metrics for text classification tasks.

\subsection{Word Length}
\begin{quote}
\textit{“Because even the smallest of words can be the ones to hurt you, or save you.”} -- Natsuki Takaya 
\end{quote}
Word length fills in the set of trivial metrics we implement for text benchmarking. The intuition is simple. Given a sufficiently large corpus, the average word length -- that is, the number of characters in a word -- will converge to a certain number -- in English, this number tends to be between 4 and 5. Any deviations, either positive or negative, from this norm can then be used to determine the complexity of the text. For example, a text with a large number of long words is likely to be more complex than a text with a large number of short words. Naturally, words expressing more specific concepts tend to have a longer character length than words we use in general speech and are sometimes ambiguous. This phenomenon is established in English, although the essence may not generalize well for other languages, e.g. Chinese and Japanese, where a single character can generalize to a whole word or a concept as a whole, but given that we are working in the context of the English language, we are not concerned with this issue.

\subsection{Sentence Length}
Similar to word length above, the average sentence length is a trivial metric describing the number of characters per sentence. Intuition tells us it will be closely related to the average word length, but also indicative of text features such as complexity and readability. For example, legal documents tend to have longer sentences than, say, newspaper articles. This is because legal documents tend to be more complex and require more time to read and understand. In contrast, newspaper articles tend to be more accessible and are written in a way that is easy to understand. 

Writers may also be interested in this metric, as very long sentences are often difficult to read and understand, as the reader may lose track of the subject of the sentence among the many objects, actions and modifiers; not to mention unnecessary punctuation where simply beginning a new sentence would be far more readable... a useful feature like this can pinpoint such writing issues, inform writers where they may cut or simplify their sentences, and in general help them improve their writing style -- a feature that is often overlooked in the context of text understanding -- this is also the longest sentence in the entire document.

\subsection{Number of Tokens}
Completing the set of trivial metrics is the general number of tokens in the text. The metric correlates highly with average sentence length and word length. Rather than counting characters in the sentence or word length, however, we take a look at the number of tokens encountered in the text, usually at the sentence level. 

\subsection{Concreteness}
\label{concreteness}
Concreteness is the degree to which a word refers to a tangible object or a concrete idea. For example, the word \textit{apple} is concrete, while the word \textit{time} is abstract. \cite{brysbaert2014concreteness} provide a dataset of concreteness ratings for 40,000 English lemmas (English words and 2,896 two-word expressions (such as ``zebra crossing'' and ``zoom in''), obtained from over four thousand participants by means of a norming study using internet crowdsourcing for data collection). The dataset is based on the concreteness ratings of the four thousand participants, who rated the concreteness of 40,000 words on a scale from 1 to 5.
The concreteness of a word is measured on a scale from 1 to 5, where 1 is the most abstract and 5 is the most concrete: 

\mk{direct citation of the study, if i need to paraphrase it, probably would delete it}
\begin{quote}
    Some words refer to things or actions in reality, which you can experience directly through one of the five senses. We call these words concrete words. Other words refer to meanings that cannot be experienced directly but which we know because the meanings can be defined by other words. These are abstract words. Still other words fall in-between the two extremes, because we can experience them to some extent and in addition we rely on language to understand them. We want you to indicate how concrete the meaning of each word is for you by using a 5-point rating scale going from abstract to concrete.
\end{quote}

The dataset provides norms for the 40,000 words and 2,896 two-word expressions -- including mean and standard deviation for each entry.

The intuition of this metric is that a word that is more concrete is more likely to be used in a creative context, as it is easier to imagine and relate to. It not only describes one aspect of the word's meaning, but authors (and genres, in general), tend to exhibit specific characteristics, such as legal documents being more generally more concrete - one would expect concrete objects and entities to appear more in documents such as the UN Human Rights Charter, or protocols for health standards control, for example. 

\subsection{Imageability}
\label{sec:imageability}
Imageability is the degree to which a word evokes a mental image, as described by \cite{degroot1989representational}. For example, the word \textit{apple} is more imageable than the word \textit{time}. \cite{cortese_imageability_2004} provide a dataset of imageability ratings for 3,000 single-syllable English lemmas, obtained from over four thousand participants by means of a norming study using internet crowdsourcing for data collection. The dataset is based on the imageability ratings of the four thousand participants, who rated the imageability of the 3,000 words on a scale from 1 to 7. The dataset also contains the number of participants who rated each word, the standard deviation of the ratings, the mean and the standard deviation for the reaction time of the participants.

\subsection{Frequent Word Usage}
\label{frequency}

\begin{quote}
\textit{    “Separate text from context and all that remains is a con.”} -- Stewart Stafford 
\end{quote}

Word frequency refers to the number of times a given word appears in a given context. Word frequency naturally differs from text to text, and smart word choice in general is an excellent indicator for intellectual linguistic use. The intuition behind selecting this metric is that words that are occurring less frequently in common speech are more likely to be used in a creative context. To give an example by rewording the last sentence, would yield: ``The intuition behind identifying this linguistic measure owes to the words' property of inverse proportionality between frequency and perceived creative or intellectual value.''

As noted, less common words are associated with higher perceived intellectual value. Even more so, the use of less common collocations (words occurring very close in a given context) hints at a higher level of linguistic skill. Of course, simply chaining completely unrelated words together (e.g. ``palmarian tobaccophile ephemeron urbarial'') hints not to high intellectual value, but rather to spitting out a random sequence of words. Properly applied in context, though, commonly not associated words can be used to great effect. This is especially true in the case of poetry, where the use of uncommon words and collocations is a common practice, or, for example, in biological contexts, such as medicine and botany, where very precise yet niche namings and conventions are mandated. This type of dissonance between common speech and niche terminology is a common theme in creative writing, and is often used to great effect. For example:

\begin{quote}
    \textit{``When they'd gone the old man turned around to watch the sun's slow descent. The Boat of Millions of Years, he thought; the boat of the dying sungod Ra, tacking down the western sky to the source of the dark river that runs through the underworld from west to east, through the twelve hours of the night, at the far eastern end of which the boat will tomorrow reappear, bearing a once again youthful, newly reignited sun.''}
    \begin{flushright}
        -- \textit{The Anubis Gates}, Tim Powers
    \end{flushright}
\end{quote}
In this context, ``boat'' is a completely valid and understandable synonym of the word ``sun'', yet the word ``boat'' co-occurring with the word ``sun'' outside this context is not common, and therefore, we are prompted to believe that this context is more `creative'.

We tackle the topic of contextual surprise further on with subsequent metrics, but for now, we focus on the general idea of individual word frequency. 

Given a sufficiently large linguistic corpus, we obtain a list of words and their frequency of occurrence. We can then use this list to calculate the frequency of occurrence of a given word in a given text. We can then use this frequency as a metric for the text's creativity. Choice of corpus is key here, as the corpus should be large enough to contain a wide variety of words, but not specialized enough to inflate the frequency of niche words. For example, a corpus of medical texts would contain a lot of medical terminology, which would inflate the frequency of medical terms, and therefore, would not be a good choice for a general creativity metric, for example in the case of a poetry contest.

For our use case, we opt to use the British National Corpus (BNC) \citep{bnc-20.500.14106/2554}, which is a 100 million word collection of samples of written and spoken language from a wide range of sources, designed to represent a wide cross-section of British English from the later part of the 20th century, both spoken and written. The BNC is a good choice for our use case, as it is a general corpus, and contains a wide variety of words, but is not specialized enough to inflate the frequency of niche words. 

The frequency lists we use are provided by the work of \cite{leech_rayson_wilson_2014} and are readily available in sheet form for both lemmatized and non-lemmatized words. In our case, we attempt to adhere only to the lemmatized versions in order to have consistency with previous metrics, but also to have normalized results, e.g., although the words `am', `is', `are' are all inflections of the verb `to be', they may have different frequencies and different positions in the list. POS tagging and lemmatization again come into play here, as we need to be able to identify the lemma of a given word in order to find its proper frequency in the list. The frequency lists indicate the words' frequencies per 100 million tokens. Intuitively, given a varied enough corpus such as the BNC, we expect these numbers to normalize and generalize well for general English. We then use the frequencies for the lemmas and the take the logarithm with base 10 of the given frequency like so:

\begin{equation}
    \label{eq:frequency}
    \text{freq}(x) = \log_{10}(\text{Frequency}_{BNC\ 1M}(\text{Lemma}(x)))
\end{equation}

Like before, if a word does not appear in the BNC, we discard it and continue. We then calculate the average frequency of the words in the text, and return the metric for interpretation by the end user.

\subsection{Proportion of Parts of Speech}
\label{pos_prop}

\begin{quote}
\textit{    “The difference between the right word and the almost right word is the difference between lightning and a lightning bug.”} -- Mark Twain
\end{quote}

We hypothesize that the proportion of part of speech tags can have meaning in terms of distinguishing genre and author characteristics. For example, a text with a larger proportion of nouns is likely to be inherently different to a text containing a larger proportion of verbs or adjectives. In fiction and creative writing, we are likely to see a higher prevalence of nouns and verbs, as those tend to describe abstract concepts, or to portray some scene with characters. Legal documents may be more heavily reliant on having more nouns and adjectives, as they are more likely to be used in a descriptive context. Internet forums and social media posts are likely to have a higher proportion of verbs, as they are more likely to be used in a conversational context, and attention is more likely to be focused on the action rather than the object. In general, we expect the proportion of parts of speech to be indicative of the genre of the text, and therefore, we can use it as a metric for creativity and classifying genre.

We use the NLTK library \citep{nltk_citation} to perform part of speech tagging on the text. We then calculate the proportion of each part of speech tag in the text, and return the metric for interpretation by the end user. For most purposes, we use the universal tagset, which makes no distinction, for example, between common nouns such as `person' or `city', and proper nouns such as `Alice' and `London', and instead groups them together under the tag `NOUN'. We encourage more ambitious authors to explore the possibilities of using the more fine-grained tagsets, such as the Penn Treebank tagset.

\subsection{Predictability}\label{predictability}\label{heavyweight_metrics}

\begin{quote}
\textit{    “The most exciting phrase to hear in science, the one that heralds new discoveries, is not `Eureka!' but `That's funny...'}” -- Isaac Asimov
\end{quote}


One aspect of creativity relates to how we perceive novelty in using already existing concepts. Imagine a person who has only ever read and knows a single book. For them, any other book they read after that has the potential to be infinitely more different from the first book they know. However, a person who has read a thousand books, is likely to not perceive much novelty in most of the text of the thousand and first book they get to read. Yet, this aspect of novelty is precisely what some people may consider to be creative -- proposing novel solutions to an existing or even a non-existing problem.

Consider again the example of the person who has read a single book and the person who has read a thousand books. In a sense, this aspect of value and novelty is completely subjective and dependent on the individual's prior knowledge. However, common sense is something that a majority of the people have. This common sense means that there must be some unifying factor between people that makes them perceive novelty in a similar way. We hypothesize one such factor to be what we call the predictability of the text.

Predictability refers to how predictable words are in a given context. For example, given the sentence ``Jenny was feeling sick, so Jenny went to the \dots'', one would commonly expect the next word to be ``doctor'', as it is the most predictable word in this context. However, if the next word is ``beach'', we could say that the sentence is more creative, as it is less predictable. Naive intuition tells us that the more predictable a word is, the less creative it is. Of course, naive is the key word here, as we make some overly simplifying assumptions. Exceptions to this rule will apply, but we believe that this generalizes well for the majority of cases. 

Then, the common question would be how to detect this kind of predictability. In the trivial case, we can hypothesize a bigram, trigram, or some n-gram model that would be able to predict the next word in a given context. This n-gram model will have learned some probability distribution of the tokens, that is, given the context:
\begin{equation}
    P(\text{word}| \text{context}_1, \text{context}_2, \dots, \text{context}_{n-1}) = \text{probability of the word given the context}
\end{equation}

It will return some probability distribution for the concrete word given the context length $1, 2, \dots, n-1$. 
We can then use this probability distribution to calculate the predictability of the text. For example, given a text, we can calculate the probability of each word in the text given the context of the previous $n-1$ words. We can take the average of these probabilities, and return the metric for interpretation by the end user. That would be a similar approach to how we calculated the concreteness and imageability ratings. But we can do better, given the state-of-the-art.

Recent Large Language Models (LLMs) have shown ability to understand text even within a vast context and Transformer-based models in particular have been performing excellently in tasks such as text summarization, text generation, and question answering. We specifically turn our attention to a particular class of LLMs, called Masked Language Models (MLMs). Those are trained to predict a masked token in a given context. Given the sentence above, the model would be trained by masking each of the words in the sentence, and then predicting the masked word given the context. This leads to a model capable of understanding context to an impressive degree and predicting the next word in a given context.

We put forward the intuition that one such LLM, if trained on a broad enough subset of the English language, would be able to represent something akin to ``common sense'', in terms of the linguistic capabilities of the so-called `averaged' human.
Then, if we put the said model to work by trying to predict a word given some context, it could represent the intuition of the average human in terms of what the average human would expect to see. Therefore, instead of limiting ourselves to just some memory-hungry n-gram model, we can use a pre-trained LLM to calculate the predictability of a word within a context.

Then, how would we formulate predictability of a given word, given some context, in terms of LLMs? \textbf{We define predictability as the metric of averaged gradient over the likelihoods of the top K masked token replacement suggestions}. 

When we mask a word in a given context, the model can return a probability distribution of the most likely words to replace the masked token with, along with likelihoods for those words. We can take the list of the top K likelihoods and sort it into descending order for the values. We can then take the gradient of this list to obtain values for how the likelihood changes over the order of the likeliest words. A probability distribution that is more uniform will have a smaller gradient, while a probability distribution that is more skewed will have a larger gradient. We can then take the average of the gradient values, and return the metric for interpretation by the end user. 

In a way, predictability is a measure of the model's confidence in its predictions. A text, for which the model is more confident in its predictions, is likely to lead to less creative text, as it is more likely to predict the most likely word in a given context. On the other hand, a text, for which the model is less confident in its predictions is likely to lead to more creative text, as it is more likely to predict a less likely word in a given context.

\subsection{Surprisal}\label{surprisal}

\begin{quote}  
\textit{"Nothing is more dangerous than an idea when it is the only one you have."} -- Emile Chartier
\end{quote}

Surprisal is a metric that is closely related to predictability. In fact, surprisal follows closely the outlined algorithm for obtaining masked word suggestions given some context. The difference is that, whereas predictability deals with the model's confidence in suggesting (or, more specifically, the probability distributions), surprisal deals with the actual tokens that have been suggested. 

\textbf{We define surprisal as the averaged similarity of a given word to the top K masked token replacement suggestions.} 

Following the procedure above, we can obtain the top K masked token replacement suggestions for a given word in a given context. We can then calculate the similarity of the given word to each of the top K masked token replacement suggestions. We can then take the average of the similarity values, and return the metric for interpretation by the end user. We tackled the topic of word sense disambiguation back in Chapter \ref{chap:background} and Sections \ref{sec:wordnet} and \ref{sec:word2vec}, so we will not delve into the details of what we define as similarity here, but we use approaches in the resources above.

The name surprisal may not be very apt for the metric, as it is not a measure of how surprised a human would be to see a given word in a given context, but nonetheless it relies on the intuition that a word that tends to have nothing in common with the top K masked token replacement suggestions is likely to be more surprising than a word that tends to have a lot in common. This kind of surprise can provide hints as to potential aspect of creativity in text.
