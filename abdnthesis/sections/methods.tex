\chapter{Methodology}
\label{chap:methods}

In this chapter, we explore the different datasets to be used, the methods for evaluating creativity, and the algorithms for creativity evaluation. We will furthermore discuss the strengths and limitations of the proposed methods and algorithms, including time complexity, memory constraints, and the accuracy of the results. We take an informed approach to the selection of the datasets and the methods for evaluating creativity, and discuss the reasons for our choices in detail, and support them with relevant literature as explored by other researchers in the field.

\section{Datasets}
\label{sec:datasets}
Datasets are vital for the success of any given project in the field of machine learning, and even more so when concerning linguistics. As evidenced by \cite{torralba_unbiased_2011}, the quality of the data used for training a model has a direct impact on the quality of the results. A model trained on a specific dataset, e.g. a corpus of law documents, can be expected to perform poorly on a dataset of medical documents, as the two domains are inherently different. Thus, we take particular care in planning and selecting the datasets we use. We also consider ease of use and access, as some datasets may require additional processing, others are subject to availability issues (e.g. paid datasets and corpora), and some may be too large to be used in a reasonable amount of time.
In this section, we will explore the datasets used in this project, and discuss their strengths and limitations.
\subsection{Brown Corpus}
The Brown Corpus \citep*{francis1979brown} is a widely used corpus in the field of computational linguistics, noted for the small variety of genres of literature it contains. The Corpus itself is founded on a compilation of American English literature from the year 1961. It is also small in terms of size, totalling around one million words, at least compared to modern corpora, which we also explore later on. The corpus also suffers from the issue of recency, as the works and language may be outdated for modern speakers of English.

Of interest is the fact that the corpus has been manually tagged for parts of speech, a process that tends to be error-prone. %citation good
As we will see later on, this fact has implications in terms of the supervised learning algorithms we implement for creativity evaluation. Still, we opt to utilize it primarily for prototyping purposes and drawing preliminary conclusions about the effectiveness of the implemented algorithms, rather than in-depth analysis and publication of results.

\subsection{Project Gutenberg}
Project Gutenberg\footnote[1]{\url{https://www.gutenberg.org/}} is a large collection of more than 50,000 works available in the public domain. The collection contains literature from various years and various genres and thus is suitable for training and evaluation of the developed benchmarks in the context of creativity study. 

As the Project does not offer an easy to process copy of its collection, we turn to the work of \cite{DBLP:journals/corr/abs-1812-08092}. The team developed a catalogue for on-demand download of the entire set of books available on the Project Gutenberg website, intended for use in the study of computational linguistics. The tool avoids the overhead of writing a web-scraper or a manual parser for the downloadable collections of Project Gutenberg books made available by third parties, as well as enables easy synchronization of newly released literature. Instead, we are only required to develop a simple pipeline for the data to be fed into the utilized systems. 

\subsection{Hierarchical Neural Story Generation}
In their work, \cite{fan_hierarchical_2018} trained a language model for text generation tasks on a dataset comprised of short stories submitted by multiple users given a particular premise (a prompt or a theme) by another user. \mk{Give an example for how one such short story would look like.} The dataset in question is technically referred to a series of posts and comments (threads) to them on the popular social media platform \textsc{Reddit}, and more tightly, the \textit{subreddit} forum \textsc{r/WritingPrompts}. The authors of the work \cite{fan_hierarchical_2018} have made the dataset available for public use, and we have used it for the purpose of evaluating the performance of our creativity benchmarks. As described by the authors on their GitHub page\footnote{\url{https://github.com/facebookresearch/fairseq/blob/main/examples/stories/README.md}}, the paper models the first 1000 tokens (words) of each story.

\mk{How do we use this dataset? You should describe the process of how we use it. }

\subsection{Discarded Datasets and Corpora}
Some datasets were considered, however, discarded due to: not being deemed applicable for the context of the application; general lack of availability of the dataset in a form that is easily accessible for our purposes; simply being infeasible to use due to the size of the dataset and the hardware constraints imposed on the project; or other reasons of similar nature.

\subsubsection*{The COCA}
The Corpus of Contemporary American English (COCA)\footnote[2]{\url{https://www.english-corpora.org/coca/}} is a large corpus of American English, containing nearly 1 billion words of text from contemporary sources. It is a collection of texts from a variety of genres, including fiction, non-fiction, and academic writing. The corpus offers a variety of tools for analysis of the data, including a concordance tool, a word frequency list, and a collocation finder. Naturally, many of those tools could be used in the field of statistical creativity analysis that we explore.

The corpus does offer limited access to the full API, as well as free samples of the data, however, the full corpus is not available for free, and the cost of acquiring it is prohibitive for the limitations set forward by the project. Nevertheless, the corpus is a valuable resource for the field of computational linguistics, and we would like to explore it further given less constraints.
\section{WordNet}
WordNet\citep{wordnet1998fellbaum} is a lexical database of semantic relations between words that links words into semantic relations including synonyms, hyponyms, and meronyms. The synonyms are grouped into synsets (sets of synonyms) with short definitions and usage examples. It can thus be seen as a combination and extension of a dictionary and thesaurus \citep{enwiki:1143619785}. 

For our specific use cases, we have identified it as a valuable resource in terms of relational representation of words in semantic space. In the given context, this enables us to traverse a semantic graph for synonyms and related words for the goal of enriching potential similarity between the set of creative parts of speech (i.e., nouns, adjectives, adverbs), which we narrow down our scope to in particular. 
% this sounds a bit weird
% insert potential uses
\subsection{Numerical representations of semantic tokens}
\mk{Potentially move this section to background work}
The idea of representing words or lexical tokens as numerical vectors (or even scalars) is hardly new. 
For example the SimLex-999 dataset \citep*{hill-etal-2015-simlex} gives values on a scale from 0 to 10, like the examples below, which range from near-synonyms (vanish, disappear) to pairs that scarcely seem to have anything in common (hole, agreement):

\begin{table}[htbp]
    \centering
        \begin{tabular}{llc}
            \toprule
            word1 & word2 & score \\
            \midrule
            vanish & disappear & 9.8 \\
            hole & agreement & 1.2 \\
            \bottomrule
       \end{tabular}
    \caption{Example Simlex-999 pairs}
    \label{simlex2pairs}
\end{table}


Early work on affective meaning by \cite{osgood1957measurement} found that words varied along three important dimensions of affective meaning:
\begin{itemize}
    \item valence: the pleasantness of the stimulus
    \item arousal: the intensity of emotion provoked by the stimulus
    \item dominance: the degree of control exerted by the stimulus
\end{itemize}

\cite{osgood1957measurement} noticed that in using these 3 numbers to represent the meaning of a word, the model was representing each word as a point in a three-dimensional space, a vector whose three dimensions corresponded to the word’s rating on the three scales. This revolutionary idea that word meaning could be represented as a point in space (e.g., that part of the meaning of heartbreak can be represented as the point $[2.45,5.65,3.58]$) was the first expression of the vector semantics models that we introduce next. \mk{**You can paraphrase this**}

\subsection*{Word2Vec}

\citet*{mikolov_word2vec_2013} show in their work that words may be represented as dense vectors in $N$-dimensional space, and we can perform mathematical operations on them that may yield effective results in terms of word representation. 

\subsection*{Measuring distance in vector representations of semantic tokens}
Intuition tells us that the dot product of vectors in $N$-dimensional space will grow when the set of vectors has similar values and decrease when the values are not similar. Thus, we can then construct the following metric for semantic similarity between vector representations of words:
$$ D(v,w) = v \times w = \sum_{i=1}^{N} v_i w_i = v_1 w_1 + v_2 w_2 + \dots + v_N w_N $$ 

The current metric, however, suffers from the problem that vectors of higher dimensions will inevitably be larger than vectors with lower dimensions. Furthermore, embedding vectors for words that occur frequently in text, tend to have high values in more dimensions, that is, they correlate with more words. The proposed solution is to normalize using the \textbf{vector length} as defined:
$$ | v| = \sqrt{\sum_{i=1}^{N}v_i^2}$$

Therefore, we obtain the following:

$$ \text{Similarity} (v, w) = \frac{v \times w}{|v| |w|} = \frac{\sum_{i=1}^{N} v_i w_i}{\sqrt{\sum_{i=1}^{N}v_i^2} \sqrt{\sum_{i=1}^{N}w_i^2}}$$

This product turns out to be the same as the cosine of the angle between two vectors:

$$ \frac{a \times b}{|a| |b|} = \cos(\theta) $$

Therefore, we will call this metric the \textbf{cosine similarity} of two words. As mentioned, the similarity grows for vectors with similar features along the same dimensions. Note the boundaries of said cosine metric: we get $-1$ for vectors which are polar opposites, $0$ for orthogonal vectors, and $1$ for equivalent vectors. Of note is the fact that such learned vector embeddings only have values in the positive ranges, thus, it is impossible to have negative values for the cosine similarity (Similarity$(a,b) \in [0,1]$).

Contrary to it, we also identify the metric of \textbf{cosine distance} between two vectors, as one minus the similarity of the vectors, or:

$$ \text{Distance}(v,w) = 1 - \text{Similarity}(v,w) $$

The cosine distance may prove useful when dealing with minimisation problems as is often the case with machine learning.

\section{Metrics}
\label{sec:metrics}

\subsection{Number of Words}
The total number of words in a given piece of text. At first glance, this metric does not impress and is, in fact, exceedingly simple. But that is fine -- we do not always need complex metrics. Sometimes, even a trivial metric as this one can inform a lot about the structure of the text. For example, the number of words in a text is directly correlated with the length of the text. This can be useful in determining the complexity of the text, as well as the time it takes to read it. In some uses, for example, comparing between books and \textit{Twitter} posts, we do not need much more information to recognize that these texts belong to entirely different genres. Such a metric is a good complement to and often used in conjunction with other metrics.

\subsection{Number of Sentences}
The number of sentences, similarly to number of words, is a trivial measure for the length of the text. However, it can be used to determine the complexity of the text. For example, a text with a large number of sentences is likely to be more complex than a text with a small number of sentences. This is because a text with a large number of sentences is likely to involve longer intellectual activity. Of course, in light of recent developments in the field of natural language generation, this metric is not particularly useful. However, due to how trivial to implement it is, it can be used in conjunction with other metrics for text classification tasks.

\subsection{Word Length}
\begin{quote}
\textit{“Because even the smallest of words can be the ones to hurt you, or save you.”} -- Natsuki Takaya 
\end{quote}
Word length fills in the set of trivial metrics we implement for text benchmarking. The intuition is simple. Given a sufficiently large corpus, the average word length -- that is, the number of characters in a word -- will converge to a certain number -- in English, this number tends to be between 4 and 5. Any deviations, either positive or negative, from this norm can then be used to determine the complexity of the text. For example, a text with a large number of long words is likely to be more complex than a text with a large number of short words. Naturally, words expressing more specific concepts tend to have a longer character length than words we use in general speech and are sometimes ambiguous. This phenomenon is established in English, although the essence may not generalize well for other languages, e.g. Chinese and Japanese, where a single character can generalize to a whole word or a concept as a whole, but given that we are working in the context of the English language, we are not concerned with this issue.

\subsection{Sentence Length}
Similar to word length above, the average sentence length is a trivial metric describing the number of characters per sentence. Intuition tells us it will be closely related to the average word length, but also indicative of text features such as complexity and readability. For example, legal documents tend to have longer sentences than, say, newspaper articles. This is because legal documents tend to be more complex and require more time to read and understand. In contrast, newspaper articles tend to be more accessible and are written in a way that is easy to understand. 

Writers may also be interested in this metric, as very long sentences are often difficult to read and understand, as the reader may lose track of the subject of the sentence among the many objects, actions and modifiers; not to mention unnecessary punctuation where simply beginning a new sentence would be far more readable... a useful feature like this can pinpoint such writing issues, inform writers where they may cut or simplify their sentences, and in general help them improve their writing style -- a feature that is often overlooked in the context of text understanding -- this is also the longest sentence in the entire document.

\subsection{Number of Tokens}
Completing the set of trivial metrics is the general number of tokens in the text. The metric correlates highly with average sentence length and word length. Rather than counting characters in the sentence or word length, however, we take a look at the number of tokens encountered in the text, usually at the sentence level. 

\subsection{Concreteness}
\label{concreteness}
Concreteness is the degree to which a word refers to a tangible object or a concrete idea. For example, the word \textit{apple} is concrete, while the word \textit{time} is abstract. \cite{brysbaert2014concreteness} provide a dataset of concreteness ratings for 40,000 English lemmas (English words and 2,896 two-word expressions (such as ``zebra crossing'' and ``zoom in''), obtained from over four thousand participants by means of a norming study using internet crowdsourcing for data collection). The dataset is based on the concreteness ratings of the four thousand participants, who rated the concreteness of 40,000 words on a scale from 1 to 5.
The concreteness of a word is measured on a scale from 1 to 5, where 1 is the most abstract and 5 is the most concrete: 

\mk{direct citation of the study, if i need to paraphrase it, probably would delete it}
\begin{quote}
    Some words refer to things or actions in reality, which you can experience directly through one of the five senses. We call these words concrete words. Other words refer to meanings that cannot be experienced directly but which we know because the meanings can be defined by other words. These are abstract words. Still other words fall in-between the two extremes, because we can experience them to some extent and in addition we rely on language to understand them. We want you to indicate how concrete the meaning of each word is for you by using a 5-point rating scale going from abstract to concrete.
\end{quote}

The dataset provides norms for the 40,000 words and 2,896 two-word expressions -- including mean and standard deviation for each entry.

The intuition of this metric is that a word that is more concrete is more likely to be used in a creative context, as it is easier to imagine and relate to. It not only describes one aspect of the word's meaning, but authors (and genres, in general), tend to exhibit specific characteristics, such as legal documents being more generally more concrete - one would expect concrete objects and entities to appear more in documents such as the UN Human Rights Charter, or protocols for health standards control, for example. 

\subsection{Imageability}
\label{imageability}
Imageability is the degree to which a word evokes a mental image, as described by \cite{degroot1989representational}. For example, the word \textit{apple} is more imageable than the word \textit{time}. \cite{brysbaert2014concreteness} provide a dataset of imageability ratings for 9,000 English lemmas (English words and 2,896 two-word expressions (such as ``zebra crossing'' and ``zoom in''), obtained from over four thousand participants by means of a norming study using internet crowdsourcing for data collection). The dataset is based on the imageability ratings of the four thousand participants, who rated the imageability of 9,000 words on a scale from 1 to 5. The dataset also contains the number of participants who rated each word, and the standard deviation of the ratings.

\subsection{Frequent Word Usage}
\label{frequency}

\begin{quote}
\textit{    “Separate text from context and all that remains is a con.”}― Stewart Stafford 
\end{quote}

Word frequency refers to the number of times a given word appears in a given context. Word frequency naturally differs from text to text, and smart word choice in general is an excellent indicator for intellectual linguistic use. The intuition behind selecting this metric is that words that are occurring less frequently in common speech are more likely to be used in a creative context. To give an example by rewording the last sentence, would yield: ``The intuition behind identifying this linguistic measure owes to the words' property of inverse proportionality between frequency and perceived creative or intellectual value.''

As noted, less common words are associated with higher perceived intellectual value. Even more so, the use of less common collocations (words occurring very close in a given context) hints at a higher level of linguistic skill. Of course, simply chaining completely unrelated words together (e.g. ``palmarian tobaccophile ephemeron urbarial'') hints not to high intellectual value, but rather to spitting out a random sequence of words. Properly applied in context, though, commonly not associated words can be used to great effect. This is especially true in the case of poetry, where the use of uncommon words and collocations is a common practice, or, for example, in biological contexts, such as medicine and botany, where very precise yet niche namings and conventions are mandated. This type of dissonance between common speech and niche terminology is a common theme in creative writing, and is often used to great effect. For example:

\begin{quote}
    \textit{``When they'd gone the old man turned around to watch the sun's slow descent. The Boat of Millions of Years, he thought; the boat of the dying sungod Ra, tacking down the western sky to the source of the dark river that runs through the underworld from west to east, through the twelve hours of the night, at the far eastern end of which the boat will tomorrow reappear, bearing a once again youthful, newly reignited sun.''}
    \begin{flushright}
        -- \textit{The Anubis Gates}, Tim Powers
    \end{flushright}
\end{quote}
In this context, ``boat'' is a completely valid and understandable synonym of the word ``sun'', yet the word ``boat'' co-occurring with the word ``sun'' outside this context is not common, and therefore, we are prompted to believe that this context is more `creative'.

We tackle the topic of contextual surprise further on with subsequent metrics, but for now, we focus on the general idea of individual word frequency. 

Given a sufficiently large linguistic corpus, we obtain a list of words and their frequency of occurrence. We can then use this list to calculate the frequency of occurrence of a given word in a given text. We can then use this frequency as a metric for the text's creativity. Choice of corpus is key here, as the corpus should be large enough to contain a wide variety of words, but not specialized enough to inflate the frequency of niche words. For example, a corpus of medical texts would contain a lot of medical terminology, which would inflate the frequency of medical terms, and therefore, would not be a good choice for a general creativity metric, for example in the case of a poetry contest.

For our use case, we opt to use the British National Corpus (BNC) \citep{bnc-20.500.14106/2554}, which is a 100 million word collection of samples of written and spoken language from a wide range of sources, designed to represent a wide cross-section of British English from the later part of the 20th century, both spoken and written. The BNC is a good choice for our use case, as it is a general corpus, and contains a wide variety of words, but is not specialized enough to inflate the frequency of niche words. 

The frequency lists we use are provided by the work of \cite{leech_rayson_wilson_2014} and are readily available in sheet form for both lemmatized and non-lemmatized words. In our case, we attempt to adhere only to the lemmatized versions in order to have consistency with previous metrics, but also to have normalized results, e.g., although the words `am', `is', `are' are all inflections of the verb `to be', they may have different frequencies and different positions in the list. POS tagging and lemmatization again come into play here, as we need to be able to identify the lemma of a given word in order to find its proper frequency in the list. The frequency lists indicate the words' frequencies per 100 million tokens. Intuitively, given a varied enough corpus such as the BNC, we expect these numbers to normalize and generalize well for general English. We then use the frequencies for the lemmas and the take the logarithm with base 10 of the given frequency like so:

\begin{equation}
    \label{eq:frequency}
    \text{freq}(x) = \log_{10}(\text{Frequency}_{BNC\ 1M}(\text{Lemma}(x)))
\end{equation}

Like before, if a word does not appear in the BNC, we discard it and continue. We then calculate the average frequency of the words in the text, and return the metric for interpretation by the end user.

\subsection{Proportion of Parts of Speech}
\label{pos_prop}
