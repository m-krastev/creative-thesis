@Book{RussellNorvig2022,
  author    = {Russell, Stuart J and Norvig, Peter},
  publisher = {Prentice Hall},
  title     = {Artificial Intelligence: A Modern Approach},
  year      = {2022},
  edition   = {4th},
}

@inproceedings{Amado2022,
author = {Leonardo R. Amado and Reuth Mirsky and Felipe Meneguzzi},
title = {{Goal Recognition as Reinforcement Learning}},
booktitle = {Proceedings of the 36th AAAI Conference on Artificial Intelligence (AAAI)},
year = {2022},
publisher = {AAAI Press},
abstract = {Most approaches for goal recognition rely on specifications of the possible dynamics of the actor in the environment when pursuing a goal. These specifications suffer from two key issues. 
First, encoding these dynamics requires careful design by a domain expert, which is often not robust to noise at recognition time. 
Second, existing approaches often need costly real-time computations to reason about the likelihood of each potential goal.
In this paper, we develop a framework that combines model-free reinforcement learning and goal recognition to alleviate the need for careful, manual domain design, and the need for costly online executions.
This framework consists of two main stages: offline learning of policies or utility functions for each potential goal, and online inference. 
We provide a first instance of this framework using tabular Q-learning for the learning stage, as well as three mechanisms for the inference stage. 
The resulting instantiation achieves state-of-the-art performance against goal recognizers on standard evaluation domains and superior performance in noisy environments.}
}

@Comment{jabref-meta: databaseType:bibtex;}

@article{DBLP:journals/corr/abs-1812-08092,
  author     = {Martin Gerlach and
                Francesc Font{-}Clos},
  title      = {A standardized Project Gutenberg corpus for statistical analysis of
                natural language and quantitative linguistics},
  journal    = {CoRR},
  volume     = {abs/1812.08092},
  year       = {2018},
  url        = {http://arxiv.org/abs/1812.08092},
  eprinttype = {arXiv},
  eprint     = {1812.08092},
  timestamp  = {Sat, 23 Jan 2021 01:21:08 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1812-08092.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

Princeton University "About WordNet." WordNet. Princeton University. 2010. 
@Article{Jordanous2012,
author={Jordanous, Anna},
title={A Standardised Procedure for Evaluating Creative Systems: Computational Creativity Evaluation Based on What it is to be Creative},
journal={Cognitive Computation},
year={2012},
month={Sep},
day={01},
volume={4},
number={3},
pages={246-279},
abstract={Computational creativity is a flourishing research area, with a variety of creative systems being produced and developed. Creativity evaluation has not kept pace with system development with an evident lack of systematic evaluation of the creativity of these systems in the literature. This is partially due to difficulties in defining what it means for a computer to be creative; indeed, there is no consensus on this for human creativity, let alone its computational equivalent. This paper proposes a Standardised Procedure for Evaluating Creative Systems (SPECS). SPECS is a three-step process: stating what it means for a particular computational system to be creative, deriving and performing tests based on these statements. To assist this process, the paper offers a collection of key components of creativity, identified empirically from discussions of human and computational creativity. Using this approach, the SPECS methodology is demonstrated through a comparative case study evaluating computational creativity systems that improvise music.},
issn={1866-9964},
doi={10.1007/s12559-012-9156-1},
url={https://doi.org/10.1007/s12559-012-9156-1}
}

@article{francis1979brown,
  title   = {Brown corpus manual},
  author  = {Francis, W Nelson and Kucera, Henry},
  journal = {Letters to the Editor},
  volume  = {5},
  number  = {2},
  pages   = {7},
  year    = {1979}
}


@incollection{pierrehumbert_burstiness_2012,
  address   = {Berlin, Heidelberg},
  title     = {Burstiness of {Verbs} and {Derived} {Nouns}},
  isbn      = {978-3-642-30773-7},
  url       = {https://doi.org/10.1007/978-3-642-30773-7_8},
  abstract  = {The frequencies of words vary with the discourse context, because any given word is more relevant to some topics of discussion than to others. In the statistical natural language processing literature, the term burstiness is used to characterize the tendency of topical words to occur repeatedly in bursts, separated by lulls in which they occur more rarely This article builds on the study of word burstiness by Altmann et al. (PLoS ONE 4:e7678, 2009). The study analyzed the archive of the USENET discussion group talk.origins, developed a novel method for quantifying burstiness, and showed that the burstiness of words is strongly correlated with their semantic type (in the sense of Montague semantics). Using the same dataset, I here explore the burstiness of abstract derived nouns (such as argument) in relation to their verb stems (e.g. argue) and frequency-matched nonderived nouns (such as science). I ask whether the burstiness of the derived form is inherited from the stem along with other stem features, such as the argument structure, or whether it is determined by the deverbal suffix. Overall, derived nouns pattern just like nonderived nouns, indicating that the suffix acts like the morphological head in determining the discourse statistics. This finding is interpreted in the light of Carlson's theory of dialogue games (Carlson in Dialogue games: An approach to discourse analysis. Synthese language library, vol. 17. Reidel, Dordrecht, 1983).},
  booktitle = {Shall {We} {Play} the {Festschrift} {Game}? {Essays} on the {Occasion} of {Lauri} {Carlson}'s 60th {Birthday}},
  publisher = {Springer Berlin Heidelberg},
  author    = {Pierrehumbert, Janet B.},
  editor    = {Santos, Diana and Lindén, Krister and Ng'ang'a, Wanjiku},
  year      = {2012},
  doi       = {10.1007/978-3-642-30773-7_8},
  pages     = {99--115}
}


@misc{fan_hierarchical_2018,
  title     = {Hierarchical {Neural} {Story} {Generation}},
  url       = {http://arxiv.org/abs/1805.04833},
  abstract  = {We explore story generation: creative systems that can build coherent and ﬂuent passages of text about a topic. We collect a large dataset of 300K human-written stories paired with writing prompts from an online forum. Our dataset enables hierarchical story generation, where the model ﬁrst generates a premise, and then transforms it into a passage of text. We gain further improvements with a novel form of model fusion that improves the relevance of the story to the prompt, and adding a new gated multi-scale self-attention mechanism to model long-range context. Experiments show large improvements over strong baselines on both automated and human evaluations. Human judges prefer stories generated by our approach to those from a strong non-hierarchical model by a factor of two to one.},
  language  = {en},
  urldate   = {2023-03-10},
  publisher = {arXiv},
  author    = {Fan, Angela and Lewis, Mike and Dauphin, Yann},
  month     = may,
  year      = {2018},
  note      = {arXiv:1805.04833 [cs]},
  keywords  = {Computer Science - Computation and Language},
  file      = {Fan et al. - 2018 - Hierarchical Neural Story Generation.pdf:/Users/Matey/Zotero/storage/DSWSDELA/Fan et al. - 2018 - Hierarchical Neural Story Generation.pdf:application/pdf}
}

  @book{wordnet1998fellbaum,
  title     = {WordNet: An Electronic Lexical Database},
  author    = {Christiane Fellbaum},
  year      = {1998},
  publisher = {Bradford Books},
  url       = {https://mitpress.mit.edu/9780262561167/}
}


@misc{mikolov_word2vec_2013,
  title     = {Efficient {Estimation} of {Word} {Representations} in {Vector} {Space}},
  url       = {http://arxiv.org/abs/1301.3781},
  doi       = {10.48550/arXiv.1301.3781},
  abstract  = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
  urldate   = {2023-03-14},
  publisher = {arXiv},
  author    = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  month     = sep,
  year      = {2013},
  note      = {arXiv:1301.3781 [cs]},
  keywords  = {Computer Science - Computation and Language},
  file      = {arXiv Fulltext PDF:/Users/Matey/Zotero/storage/M6YLHVWT/Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Ve.pdf:application/pdf;arXiv.org Snapshot:/Users/Matey/Zotero/storage/XMF7XD7E/1301.html:text/html}
}

  @misc{enwiki:1143619785,
  author       = {{Wikipedia contributors}},
  title        = {WordNet --- {Wikipedia}{,} The Free Encyclopedia},
  year         = {2023},
  howpublished = {\url{https://en.wikipedia.org/w/index.php?title=WordNet&oldid=1143619785}},
  note         = {[Online; accessed 14-March-2023]}
}

  @misc{enwiki:1146375871,
  author       = {{Wikipedia contributors}},
  title        = {PyTorch --- {Wikipedia}{,} The Free Encyclopedia},
  year         = {2023},
  howpublished = {\url{https://en.wikipedia.org/w/index.php?title=PyTorch&oldid=1146375871}},
  note         = {[Online; accessed 2-April-2023]}
}

@article{hill-etal-2015-simlex,
  title     = {{S}im{L}ex-999: Evaluating Semantic Models With (Genuine) Similarity Estimation},
  author    = {Hill, Felix  and
               Reichart, Roi  and
               Korhonen, Anna},
  journal   = {Computational Linguistics},
  volume    = {41},
  number    = {4},
  month     = dec,
  year      = {2015},
  address   = {Cambridge, MA},
  publisher = {MIT Press},
  url       = {https://aclanthology.org/J15-4004},
  doi       = {10.1162/COLI_a_00237},
  pages     = {665--695}
}

@book{osgood1957measurement,
  title     = {The measurement of meaning},
  author    = {Osgood, Charles Egerton and Suci, George J and Tannenbaum, Percy H},
  year      = {1957},
  publisher = {University of Illinois press}
}

@techreport{pep8,
  author = {Guido van Rossum and Barry Warsaw and Nick Coghlan},
  title  = {Style Guide for {Python} Code},
  year   = {2001},
  type   = {PEP},
  number = {8},
  institution = {Python Software Foundation},
  url    = {https://www.python.org/dev/peps/pep-0008/}
}