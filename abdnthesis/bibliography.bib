@Book{RussellNorvig2022,
  author    = {Russell, Stuart J and Norvig, Peter},
  publisher = {Prentice Hall},
  title     = {Artificial Intelligence: A Modern Approach},
  year      = {2022},
  edition   = {4th},
}

@inproceedings{Amado2022,
author = {Leonardo R. Amado and Reuth Mirsky and Felipe Meneguzzi},
title = {{Goal Recognition as Reinforcement Learning}},
booktitle = {Proceedings of the 36th AAAI Conference on Artificial Intelligence (AAAI)},
year = {2022},
publisher = {AAAI Press},
abstract = {Most approaches for goal recognition rely on specifications of the possible dynamics of the actor in the environment when pursuing a goal. These specifications suffer from two key issues. 
First, encoding these dynamics requires careful design by a domain expert, which is often not robust to noise at recognition time. 
Second, existing approaches often need costly real-time computations to reason about the likelihood of each potential goal.
In this paper, we develop a framework that combines model-free reinforcement learning and goal recognition to alleviate the need for careful, manual domain design, and the need for costly online executions.
This framework consists of two main stages: offline learning of policies or utility functions for each potential goal, and online inference. 
We provide a first instance of this framework using tabular Q-learning for the learning stage, as well as three mechanisms for the inference stage. 
The resulting instantiation achieves state-of-the-art performance against goal recognizers on standard evaluation domains and superior performance in noisy environments.}
}

@Comment{jabref-meta: databaseType:bibtex;}

@article{DBLP:journals/corr/abs-1812-08092,
  author     = {Martin Gerlach and
                Francesc Font{-}Clos},
  title      = {A standardized Project Gutenberg corpus for statistical analysis of
                natural language and quantitative linguistics},
  journal    = {CoRR},
  volume     = {abs/1812.08092},
  year       = {2018},
  url        = {http://arxiv.org/abs/1812.08092},
  eprinttype = {arXiv},
  eprint     = {1812.08092},
  timestamp  = {Sat, 23 Jan 2021 01:21:08 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1812-08092.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

Princeton University "About WordNet." WordNet. Princeton University. 2010. 
@Article{Jordanous2012,
author={Jordanous, Anna},
title={A Standardised Procedure for Evaluating Creative Systems: Computational Creativity Evaluation Based on What it is to be Creative},
journal={Cognitive Computation},
year={2012},
month={Sep},
day={01},
volume={4},
number={3},
pages={246-279},
abstract={Computational creativity is a flourishing research area, with a variety of creative systems being produced and developed. Creativity evaluation has not kept pace with system development with an evident lack of systematic evaluation of the creativity of these systems in the literature. This is partially due to difficulties in defining what it means for a computer to be creative; indeed, there is no consensus on this for human creativity, let alone its computational equivalent. This paper proposes a Standardised Procedure for Evaluating Creative Systems (SPECS). SPECS is a three-step process: stating what it means for a particular computational system to be creative, deriving and performing tests based on these statements. To assist this process, the paper offers a collection of key components of creativity, identified empirically from discussions of human and computational creativity. Using this approach, the SPECS methodology is demonstrated through a comparative case study evaluating computational creativity systems that improvise music.},
issn={1866-9964},
doi={10.1007/s12559-012-9156-1},
url={https://doi.org/10.1007/s12559-012-9156-1}
}

@article{francis1979brown,
  title   = {Brown corpus manual},
  author  = {Francis, W Nelson and Kucera, Henry},
  journal = {Letters to the Editor},
  volume  = {5},
  number  = {2},
  pages   = {7},
  year    = {1979}
}


@incollection{pierrehumbert_burstiness_2012,
  address   = {Berlin, Heidelberg},
  title     = {Burstiness of {Verbs} and {Derived} {Nouns}},
  isbn      = {978-3-642-30773-7},
  url       = {https://doi.org/10.1007/978-3-642-30773-7_8},
  abstract  = {The frequencies of words vary with the discourse context, because any given word is more relevant to some topics of discussion than to others. In the statistical natural language processing literature, the term burstiness is used to characterize the tendency of topical words to occur repeatedly in bursts, separated by lulls in which they occur more rarely This article builds on the study of word burstiness by Altmann et al. (PLoS ONE 4:e7678, 2009). The study analyzed the archive of the USENET discussion group talk.origins, developed a novel method for quantifying burstiness, and showed that the burstiness of words is strongly correlated with their semantic type (in the sense of Montague semantics). Using the same dataset, I here explore the burstiness of abstract derived nouns (such as argument) in relation to their verb stems (e.g. argue) and frequency-matched nonderived nouns (such as science). I ask whether the burstiness of the derived form is inherited from the stem along with other stem features, such as the argument structure, or whether it is determined by the deverbal suffix. Overall, derived nouns pattern just like nonderived nouns, indicating that the suffix acts like the morphological head in determining the discourse statistics. This finding is interpreted in the light of Carlson's theory of dialogue games (Carlson in Dialogue games: An approach to discourse analysis. Synthese language library, vol. 17. Reidel, Dordrecht, 1983).},
  booktitle = {Shall {We} {Play} the {Festschrift} {Game}? {Essays} on the {Occasion} of {Lauri} {Carlson}'s 60th {Birthday}},
  publisher = {Springer Berlin Heidelberg},
  author    = {Pierrehumbert, Janet B.},
  editor    = {Santos, Diana and Lindén, Krister and Ng'ang'a, Wanjiku},
  year      = {2012},
  doi       = {10.1007/978-3-642-30773-7_8},
  pages     = {99--115}
}


@misc{fan_hierarchical_2018,
  title     = {Hierarchical {Neural} {Story} {Generation}},
  url       = {http://arxiv.org/abs/1805.04833},
  abstract  = {We explore story generation: creative systems that can build coherent and ﬂuent passages of text about a topic. We collect a large dataset of 300K human-written stories paired with writing prompts from an online forum. Our dataset enables hierarchical story generation, where the model ﬁrst generates a premise, and then transforms it into a passage of text. We gain further improvements with a novel form of model fusion that improves the relevance of the story to the prompt, and adding a new gated multi-scale self-attention mechanism to model long-range context. Experiments show large improvements over strong baselines on both automated and human evaluations. Human judges prefer stories generated by our approach to those from a strong non-hierarchical model by a factor of two to one.},
  language  = {en},
  urldate   = {2023-03-10},
  publisher = {arXiv},
  author    = {Fan, Angela and Lewis, Mike and Dauphin, Yann},
  month     = may,
  year      = {2018},
  note      = {arXiv:1805.04833 [cs]},
  keywords  = {Computer Science - Computation and Language},
  file      = {Fan et al. - 2018 - Hierarchical Neural Story Generation.pdf:/Users/Matey/Zotero/storage/DSWSDELA/Fan et al. - 2018 - Hierarchical Neural Story Generation.pdf:application/pdf}
}

  @book{wordnet1998fellbaum,
  title     = {WordNet: An Electronic Lexical Database},
  author    = {Christiane Fellbaum},
  year      = {1998},
  publisher = {Bradford Books},
  url       = {https://mitpress.mit.edu/9780262561167/}
}


@misc{mikolov_word2vec_2013,
  title     = {Efficient {Estimation} of {Word} {Representations} in {Vector} {Space}},
  url       = {http://arxiv.org/abs/1301.3781},
  doi       = {10.48550/arXiv.1301.3781},
  abstract  = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
  urldate   = {2023-03-14},
  publisher = {arXiv},
  author    = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  month     = sep,
  year      = {2013},
  note      = {arXiv:1301.3781 [cs]},
  keywords  = {Computer Science - Computation and Language},
  file      = {arXiv Fulltext PDF:/Users/Matey/Zotero/storage/M6YLHVWT/Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Ve.pdf:application/pdf;arXiv.org Snapshot:/Users/Matey/Zotero/storage/XMF7XD7E/1301.html:text/html}
}

  @misc{enwiki:1143619785,
  author       = {{Wikipedia contributors}},
  title        = {WordNet --- {Wikipedia}{,} The Free Encyclopedia},
  year         = {2023},
  howpublished = {\url{https://en.wikipedia.org/w/index.php?title=WordNet&oldid=1143619785}},
  note         = {[Online; accessed 14-March-2023]}
}

  @misc{enwiki:1146375871,
  author       = {{Wikipedia contributors}},
  title        = {PyTorch --- {Wikipedia}{,} The Free Encyclopedia},
  year         = {2023},
  howpublished = {\url{https://en.wikipedia.org/w/index.php?title=PyTorch&oldid=1146375871}},
  note         = {[Online; accessed 2-April-2023]}
}

@article{hill-etal-2015-simlex,
  title     = {{S}im{L}ex-999: Evaluating Semantic Models With (Genuine) Similarity Estimation},
  author    = {Hill, Felix  and
               Reichart, Roi  and
               Korhonen, Anna},
  journal   = {Computational Linguistics},
  volume    = {41},
  number    = {4},
  month     = dec,
  year      = {2015},
  address   = {Cambridge, MA},
  publisher = {MIT Press},
  url       = {https://aclanthology.org/J15-4004},
  doi       = {10.1162/COLI_a_00237},
  pages     = {665--695}
}

@book{osgood1957measurement,
  title     = {The measurement of meaning},
  author    = {Osgood, Charles Egerton and Suci, George J and Tannenbaum, Percy H},
  year      = {1957},
  publisher = {University of Illinois press}
}

@techreport{pep8,
  author = {Guido van Rossum and Barry Warsaw and Nick Coghlan},
  title  = {Style Guide for {Python} Code},
  year   = {2001},
  type   = {PEP},
  number = {8},
  institution = {Python Software Foundation},
  url    = {https://www.python.org/dev/peps/pep-0008/}
}


@article{steinberger_overview_2014,
  title    = {An overview of the {European} {Union}’s highly multilingual parallel corpora},
  volume   = {48},
  issn     = {1574-0218},
  url      = {https://doi.org/10.1007/s10579-014-9277-0},
  doi      = {10.1007/s10579-014-9277-0},
  number   = {4},
  journal  = {Language Resources and Evaluation},
  author   = {Steinberger, Ralf and Ebrahim, Mohamed and Poulis, Alexandros and Carrasco-Benitez, Manuel and Schlüter, Patrick and Przybyszewski, Marek and Gilbro, Signe},
  month    = dec,
  year     = {2014},
  pages    = {679--707}
}


%
%  ProjectPlan.bib - source text for project plan bibliography
%
@misc{mirowski_co-writing_2022,
  title      = {Co-{Writing} {Screenplays} and {Theatre} {Scripts} with {Language} {Models}: {An} {Evaluation} by {Industry} {Professionals}},
  shorttitle = {Co-{Writing} {Screenplays} and {Theatre} {Scripts} with {Language} {Models}},
  url        = {http://arxiv.org/abs/2209.14958},
  language   = {en},
  urldate    = {2023-01-31},
  publisher  = {arXiv},
  author     = {Mirowski, Piotr and Mathewson, Kory W. and Pittman, Jaylen and Evans, Richard},
  month      = sep,
  year       = {2022},
  note       = {arXiv:2209.14958 [cs]},
  keywords   = {Computer Science - Computation and Language, Computer Science - Human-Computer Interaction}
}

@misc{codex_2021_copilot,
  doi       = {10.48550/ARXIV.2107.03374},
  url       = {https://arxiv.org/abs/2107.03374},
  author    = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and Ray, Alex and Puri, Raul and Krueger, Gretchen and Petrov, Michael and Khlaaf, Heidy and Sastry, Girish and Mishkin, Pamela and Chan, Brooke and Gray, Scott and Ryder, Nick and Pavlov, Mikhail and Power, Alethea and Kaiser, Lukasz and Bavarian, Mohammad and Winter, Clemens and Tillet, Philippe and Such, Felipe Petroski and Cummings, Dave and Plappert, Matthias and Chantzis, Fotios and Barnes, Elizabeth and Herbert-Voss, Ariel and Guss, William Hebgen and Nichol, Alex and Paino, Alex and Tezak, Nikolas and Tang, Jie and Babuschkin, Igor and Balaji, Suchir and Jain, Shantanu and Saunders, William and Hesse, Christopher and Carr, Andrew N. and Leike, Jan and Achiam, Josh and Misra, Vedant and Morikawa, Evan and Radford, Alec and Knight, Matthew and Brundage, Miles and Murati, Mira and Mayer, Katie and Welinder, Peter and McGrew, Bob and Amodei, Dario and McCandlish, Sam and Sutskever, Ilya and Zaremba, Wojciech},
  keywords  = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Evaluating Large Language Models Trained on Code},
  publisher = {arXiv},
  year      = {2021},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@book{nltk_citation,
  abstract  = {This book offers a highly accessible introduction to Natural Language Processing, the field that underpins a variety of language technologies ranging from predictive text and email filtering to automatic summarization and translation. Using NLTK, you'll learn how to write Python programs to analyze the structure and meaning of texts, drawing on techniques from the fields of linguistics and artificial intelligence.},
  added-at  = {2016-12-06T16:29:36.000+0100},
  address   = {Beijing},
  author    = {Bird, Steven and Klein, Ewan and Loper, Edward},
  biburl    = {https://www.bibsonomy.org/bibtex/2c90dc59441d01c8bef58a947274164d4/flint63},
  doi       = {http://my.safaribooksonline.com/9780596516499},
  file      = {O'Reilly eBook:2009/BirdKleinLoper09.pdf:PDF;O'Reilly Product page:http\://shop.oreilly.com/product/9780596516499.do:URL;Related Web Site:http\://www.nltk.org/:URL;Safari:https\://www.safaribooksonline.com/library/view/natural-language-processing/9780596803346/:URL},
  groups    = {public},
  interhash = {5408d7da097b9cd81239c238da8bfaf4},
  intrahash = {c90dc59441d01c8bef58a947274164d4},
  isbn      = {978-0-596-51649-9},
  keywords  = {01841 103 book safari ai software development language processing text python framework},
  publisher = {O'Reilly},
  timestamp = {2018-04-16T12:35:20.000+0200},
  title     = {Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit},
  url       = {http://www.nltk.org/book},
  username  = {flint63},
  year      = 2009
}

@inproceedings{mostafazadeh-etal-2016-corpus,
  title     = {A Corpus and Cloze Evaluation for Deeper Understanding of Commonsense Stories},
  author    = {Mostafazadeh, Nasrin  and
               Chambers, Nathanael  and
               He, Xiaodong  and
               Parikh, Devi  and
               Batra, Dhruv  and
               Vanderwende, Lucy  and
               Kohli, Pushmeet  and
               Allen, James},
  booktitle = {Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies},
  month     = jun,
  year      = {2016},
  address   = {San Diego, California},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/N16-1098},
  doi       = {10.18653/v1/N16-1098},
  pages     = {839--849}
}

@article{gutenberg_dataset,
  author     = {Martin Gerlach and
                Francesc Font{-}Clos},
  title      = {A standardized {P}roject {G}utenberg corpus for statistical analysis of
                natural language and quantitative linguistics},
  journal    = {CoRR},
  volume     = {abs/1812.08092},
  year       = {2018},
  url        = {http://arxiv.org/abs/1812.08092},
  eprinttype = {arXiv},
  eprint     = {1812.08092},
  timestamp  = {Sat, 23 Jan 2021 01:21:08 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1812-08092.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}


@misc{usf_word_associations,
  author = {Nelson, D. L. and McEvoy, C. L. and Schreiber, T. A.},
  title  = {The University of South Florida word association, rhyme, and word fragment norms},
  year   = {1998},
  url    = {http://www.usf.edu/FreeAssociation/}
}

Princeton University "About WordNet." WordNet. Princeton University. 2010. 
Web. 20 Apr. 2010. <http://wordnet.princeton.edu/>.
@misc{wordnet_princeton,
  author = {Princeton University},
  title  = {About {W}ord{N}et},
  year   = {2010},
  url    = {http://wordnet.princeton.edu/}
}

@incollection{NEURIPS2019_9015_pytorch,
  title     = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  author    = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  booktitle = {Advances in Neural Information Processing Systems 32},
  pages     = {8024--8035},
  year      = {2019},
  publisher = {Curran Associates, Inc.},
  url       = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}


@misc{vaswani_attention_2017,
  title     = {Attention {Is} {All} {You} {Need}},
  url       = {http://arxiv.org/abs/1706.03762},
  abstract  = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  language  = {en},
  urldate   = {2023-02-09},
  publisher = {arXiv},
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  month     = dec,
  year      = {2017},
  note      = {arXiv:1706.03762 [cs]},
  keywords  = {Computer Science - Computation and Language, Computer Science - Machine Learning},
  annote    = {Comment: 15 pages, 5 figures},
  file      = {Vaswani et al. - 2017 - Attention Is All You Need.pdf:/home/matey/Zotero/storage/8DVU7EZZ/Vaswani et al. - 2017 - Attention Is All You Need.pdf:application/pdf}
}

% Information rate in discourse may be different in creative writing: here is some work on analysing discourse
@inproceedings{giulianelli2021analysing,
  title     = {Analysing human strategies of information transmission as a function of discourse context},
  author    = {Giulianelli, Mario and Fern{\'a}ndez, Raquel},
  booktitle = {Proceedings of the 25th Conference on Computational Natural Language Learning},
  pages     = {647--660},
  year      = {2021}
}

@article{kuperman2012age,
  title     = {Age-of-acquisition ratings for 30,000 English words},
  author    = {Kuperman, Victor and Stadthagen-Gonzalez, Hans and Brysbaert, Marc},
  journal   = {Behavior research methods},
  volume    = {44},
  pages     = {978--990},
  year      = {2012},
  publisher = {Springer}
}

@article{brysbaert2014concreteness,
  title     = {Concreteness ratings for 40 thousand generally known English word lemmas},
  author    = {Brysbaert, Marc and Warriner, Amy Beth and Kuperman, Victor},
  journal   = {Behavior research methods},
  volume    = {46},
  pages     = {904--911},
  year      = {2014},
  publisher = {Springer}
}

@article{thepile_dataset,
  author     = {Leo Gao and
                Stella Biderman and
                Sid Black and
                Laurence Golding and
                Travis Hoppe and
                Charles Foster and
                Jason Phang and
                Horace He and
                Anish Thite and
                Noa Nabeshima and
                Shawn Presser and
                Connor Leahy},
  title      = {The Pile: An 800GB Dataset of Diverse Text for Language Modeling},
  journal    = {CoRR},
  volume     = {abs/2101.00027},
  year       = {2021},
  url        = {https://arxiv.org/abs/2101.00027},
  eprinttype = {arXiv},
  eprint     = {2101.00027},
  timestamp  = {Thu, 14 Oct 2021 09:16:12 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2101-00027.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}


@article{sphere_corpus,
  author     = {Aleksandra Piktus and Fabio Petroni
                and Vladimir Karpukhin and Dmytro Okhonko
                and Samuel Broscheit and Gautier Izacard
                and Patrick Lewis and Barlas Oguz
                and Edouard Grave and Wen{-}tau Yih
                and Sebastian Riedel},
  title      = {The Web Is Your Oyster - Knowledge-Intensive {NLP} against a Very
                Large Web Corpus},
  journal    = {CoRR},
  volume     = {abs/2112.09924},
  year       = {2021},
  url        = {https://arxiv.org/abs/2112.09924},
  eprinttype = {arXiv},
  eprint     = {2112.09924},
  timestamp  = {Tue, 04 Jan 2022 15:59:27 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2112-09924.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{broad_twitter,
  title     = {Broad {T}witter Corpus: A Diverse Named Entity Recognition Resource},
  author    = {Derczynski, Leon  and
               Bontcheva, Kalina  and
               Roberts, Ian},
  booktitle = {Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers},
  month     = dec,
  year      = {2016},
  address   = {Osaka, Japan},
  publisher = {The COLING 2016 Organizing Committee},
  url       = {https://aclanthology.org/C16-1111},
  pages     = {1169--1179},
  abstract  = {One of the main obstacles, hampering method development and comparative evaluation of named entity recognition in social media, is the lack of a sizeable, diverse, high quality annotated corpus, analogous to the CoNLL{'}2003 news dataset. For instance, the biggest Ritter tweet corpus is only 45,000 tokens {--} a mere 15{\%} the size of CoNLL{'}2003. Another major shortcoming is the lack of temporal, geographic, and author diversity. This paper introduces the Broad Twitter Corpus (BTC), which is not only significantly bigger, but sampled across different regions, temporal periods, and types of Twitter users. The gold-standard named entity annotations are made by a combination of NLP experts and crowd workers, which enables us to harness crowd recall while maintaining high quality. We also measure the entity drift observed in our dataset (i.e. how entity representation varies over time), and compare to newswire. The corpus is released openly, including source text and intermediate annotations.}
}



@misc{brown_gpt3_2020,
  title     = {Language {Models} are {Few}-{Shot} {Learners}},
  url       = {http://arxiv.org/abs/2005.14165},
  doi       = {10.48550/arXiv.2005.14165},
  abstract  = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  urldate   = {2023-02-10},
  publisher = {arXiv},
  author    = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  month     = jul,
  year      = {2020},
  note      = {arXiv:2005.14165 [cs]},
  keywords  = {Computer Science - Computation and Language},
  annote    = {Comment: 40+32 pages},
  file      = {arXiv Fulltext PDF:/home/matey/Zotero/storage/PXVBS5HA/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf:application/pdf;arXiv.org Snapshot:/home/matey/Zotero/storage/S4EAQKRN/2005.html:text/html}
}


@misc{zhang_opt_2022,
  title      = {{OPT}: {Open} {Pre}-trained {Transformer} {Language} {Models}},
  shorttitle = {{OPT}},
  url        = {http://arxiv.org/abs/2205.01068},
  doi        = {10.48550/arXiv.2205.01068},
  abstract   = {Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.},
  urldate    = {2023-02-10},
  publisher  = {arXiv},
  author     = {Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and Mihaylov, Todor and Ott, Myle and Shleifer, Sam and Shuster, Kurt and Simig, Daniel and Koura, Punit Singh and Sridhar, Anjali and Wang, Tianlu and Zettlemoyer, Luke},
  month      = jun,
  year       = {2022},
  note       = {arXiv:2205.01068 [cs]},
  keywords   = {Computer Science - Computation and Language, Computer Science - Machine Learning},
  file       = {arXiv Fulltext PDF:/home/matey/Zotero/storage/KKLG6D68/Zhang et al. - 2022 - OPT Open Pre-trained Transformer Language Models.pdf:application/pdf;arXiv.org Snapshot:/home/matey/Zotero/storage/JSHMZHN7/2205.html:text/html}
}


@inproceedings{torralba_unbiased_2011,
  address   = {Colorado Springs, CO, USA},
  title     = {Unbiased look at dataset bias},
  isbn      = {978-1-4577-0394-2},
  url       = {http://ieeexplore.ieee.org/document/5995347/},
  doi       = {10.1109/CVPR.2011.5995347},
  urldate   = {2023-04-12},
  booktitle = {{CVPR} 2011},
  publisher = {IEEE},
  author    = {Torralba, Antonio and Efros, Alexei A.},
  month     = jun,
  year      = {2011},
  pages     = {1521--1528},
  file      = {Submitted Version:/Users/Matey/Zotero/storage/6TVE3LLG/Torralba and Efros - 2011 - Unbiased look at dataset bias.pdf:application/pdf}
}


@article{jozefowicz_exploring_2016,
  title     = {Exploring the {Limits} of {Language} {Modeling}},
  copyright = {arXiv.org perpetual, non-exclusive license},
  url       = {https://arxiv.org/abs/1602.02410},
  doi       = {10.48550/ARXIV.1602.02410},
  urldate   = {2023-04-12},
  author    = {Jozefowicz, Rafal and Vinyals, Oriol and Schuster, Mike and Shazeer, Noam and Wu, Yonghui},
  year      = {2016},
  note      = {Publisher: arXiv
               Version Number: 2},
  keywords  = {Computation and Language (cs.CL), FOS: Computer and information sciences}
}


@misc{franceschelli_deepcreativity_2022,
  title      = {{DeepCreativity}: {Measuring} {Creativity} with {Deep} {Learning} {Techniques}},
  shorttitle = {{DeepCreativity}},
  url        = {http://arxiv.org/abs/2201.06118},
  language   = {en},
  urldate    = {2023-02-09},
  publisher  = {arXiv},
  author     = {Franceschelli, Giorgio and Musolesi, Mirco},
  month      = jan,
  year       = {2022},
  note       = {arXiv:2201.06118 [cs]},
  keywords   = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computers and Society},
  annote     = {Comment: 12 pages, 2 figures},
  file       = {Franceschelli and Musolesi - 2022 - DeepCreativity Measuring Creativity with Deep Lea.pdf:/Users/Matey/Zotero/storage/WU3Q3F9L/Franceschelli and Musolesi - 2022 - DeepCreativity Measuring Creativity with Deep Lea.pdf:application/pdf}
}
