
%
%  ProjectPlan.bib - source text for project plan bibliography
%
@misc{mirowski_co-writing_2022,
  title      = {Co-{Writing} {Screenplays} and {Theatre} {Scripts} with {Language} {Models}: {An} {Evaluation} by {Industry} {Professionals}},
  shorttitle = {Co-{Writing} {Screenplays} and {Theatre} {Scripts} with {Language} {Models}},
  url        = {http://arxiv.org/abs/2209.14958},
  language   = {en},
  urldate    = {2023-01-31},
  publisher  = {arXiv},
  author     = {Mirowski, Piotr and Mathewson, Kory W. and Pittman, Jaylen and Evans, Richard},
  month      = sep,
  year       = {2022},
  note       = {arXiv:2209.14958 [cs]},
  keywords   = {Computer Science - Computation and Language, Computer Science - Human-Computer Interaction}
}

@misc{codex_2021_copilot,
  doi       = {10.48550/ARXIV.2107.03374},
  url       = {https://arxiv.org/abs/2107.03374},
  author    = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and Ray, Alex and Puri, Raul and Krueger, Gretchen and Petrov, Michael and Khlaaf, Heidy and Sastry, Girish and Mishkin, Pamela and Chan, Brooke and Gray, Scott and Ryder, Nick and Pavlov, Mikhail and Power, Alethea and Kaiser, Lukasz and Bavarian, Mohammad and Winter, Clemens and Tillet, Philippe and Such, Felipe Petroski and Cummings, Dave and Plappert, Matthias and Chantzis, Fotios and Barnes, Elizabeth and Herbert-Voss, Ariel and Guss, William Hebgen and Nichol, Alex and Paino, Alex and Tezak, Nikolas and Tang, Jie and Babuschkin, Igor and Balaji, Suchir and Jain, Shantanu and Saunders, William and Hesse, Christopher and Carr, Andrew N. and Leike, Jan and Achiam, Josh and Misra, Vedant and Morikawa, Evan and Radford, Alec and Knight, Matthew and Brundage, Miles and Murati, Mira and Mayer, Katie and Welinder, Peter and McGrew, Bob and Amodei, Dario and McCandlish, Sam and Sutskever, Ilya and Zaremba, Wojciech},
  keywords  = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Evaluating Large Language Models Trained on Code},
  publisher = {arXiv},
  year      = {2021},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@book{nltk_citation,
  abstract  = {This book offers a highly accessible introduction to Natural Language Processing, the field that underpins a variety of language technologies ranging from predictive text and email filtering to automatic summarization and translation. Using NLTK, you'll learn how to write Python programs to analyze the structure and meaning of texts, drawing on techniques from the fields of linguistics and artificial intelligence.},
  added-at  = {2016-12-06T16:29:36.000+0100},
  address   = {Beijing},
  author    = {Bird, Steven and Klein, Ewan and Loper, Edward},
  biburl    = {https://www.bibsonomy.org/bibtex/2c90dc59441d01c8bef58a947274164d4/flint63},
  doi       = {http://my.safaribooksonline.com/9780596516499},
  file      = {O'Reilly eBook:2009/BirdKleinLoper09.pdf:PDF;O'Reilly Product page:http\://shop.oreilly.com/product/9780596516499.do:URL;Related Web Site:http\://www.nltk.org/:URL;Safari:https\://www.safaribooksonline.com/library/view/natural-language-processing/9780596803346/:URL},
  groups    = {public},
  interhash = {5408d7da097b9cd81239c238da8bfaf4},
  intrahash = {c90dc59441d01c8bef58a947274164d4},
  isbn      = {978-0-596-51649-9},
  keywords  = {01841 103 book safari ai software development language processing text python framework},
  publisher = {O'Reilly},
  timestamp = {2018-04-16T12:35:20.000+0200},
  title     = {Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit},
  url       = {http://www.nltk.org/book},
  username  = {flint63},
  year      = 2009
}

@inproceedings{mostafazadeh-etal-2016-corpus,
  title     = {A Corpus and Cloze Evaluation for Deeper Understanding of Commonsense Stories},
  author    = {Mostafazadeh, Nasrin  and
               Chambers, Nathanael  and
               He, Xiaodong  and
               Parikh, Devi  and
               Batra, Dhruv  and
               Vanderwende, Lucy  and
               Kohli, Pushmeet  and
               Allen, James},
  booktitle = {Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies},
  month     = jun,
  year      = {2016},
  address   = {San Diego, California},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/N16-1098},
  doi       = {10.18653/v1/N16-1098},
  pages     = {839--849}
}

@article{gutenberg_dataset,
  author     = {Martin Gerlach and
                Francesc Font{-}Clos},
  title      = {A standardized {P}roject {G}utenberg corpus for statistical analysis of
                natural language and quantitative linguistics},
  journal    = {CoRR},
  volume     = {abs/1812.08092},
  year       = {2018},
  url        = {http://arxiv.org/abs/1812.08092},
  eprinttype = {arXiv},
  eprint     = {1812.08092},
  timestamp  = {Sat, 23 Jan 2021 01:21:08 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1812-08092.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}


@misc{usf_word_associations,
  author = {Nelson, D. L. and McEvoy, C. L. and Schreiber, T. A.},
  title  = {The University of South Florida word association, rhyme, and word fragment norms},
  year   = {1998},
  url    = {http://www.usf.edu/FreeAssociation/}
}

Princeton University "About WordNet." WordNet. Princeton University. 2010. 
Web. 20 Apr. 2010. <http://wordnet.princeton.edu/>.
@misc{wordnet_princeton,
  author = {Princeton University},
  title  = {About {W}ord{N}et},
  year   = {2010},
  url    = {http://wordnet.princeton.edu/}
}

@incollection{NEURIPS2019_9015_pytorch,
  title     = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  author    = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  booktitle = {Advances in Neural Information Processing Systems 32},
  pages     = {8024--8035},
  year      = {2019},
  publisher = {Curran Associates, Inc.},
  url       = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}


@misc{vaswani_attention_2017,
  title     = {Attention {Is} {All} {You} {Need}},
  url       = {http://arxiv.org/abs/1706.03762},
  abstract  = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiÔ¨Åcantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  language  = {en},
  urldate   = {2023-02-09},
  publisher = {arXiv},
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  month     = dec,
  year      = {2017},
  note      = {arXiv:1706.03762 [cs]},
  keywords  = {Computer Science - Computation and Language, Computer Science - Machine Learning},
  annote    = {Comment: 15 pages, 5 figures},
  file      = {Vaswani et al. - 2017 - Attention Is All You Need.pdf:/home/matey/Zotero/storage/8DVU7EZZ/Vaswani et al. - 2017 - Attention Is All You Need.pdf:application/pdf}
}

% Information rate in discourse may be different in creative writing: here is some work on analysing discourse
@inproceedings{giulianelli2021analysing,
  title     = {Analysing human strategies of information transmission as a function of discourse context},
  author    = {Giulianelli, Mario and Fern{\'a}ndez, Raquel},
  booktitle = {Proceedings of the 25th Conference on Computational Natural Language Learning},
  pages     = {647--660},
  year      = {2021}
}

@article{kuperman2012age,
  title     = {Age-of-acquisition ratings for 30,000 English words},
  author    = {Kuperman, Victor and Stadthagen-Gonzalez, Hans and Brysbaert, Marc},
  journal   = {Behavior research methods},
  volume    = {44},
  pages     = {978--990},
  year      = {2012},
  publisher = {Springer}
}

@article{brysbaert2014concreteness,
  title     = {Concreteness ratings for 40 thousand generally known English word lemmas},
  author    = {Brysbaert, Marc and Warriner, Amy Beth and Kuperman, Victor},
  journal   = {Behavior research methods},
  volume    = {46},
  pages     = {904--911},
  year      = {2014},
  publisher = {Springer}
}

@article{thepile_dataset,
  author     = {Leo Gao and
                Stella Biderman and
                Sid Black and
                Laurence Golding and
                Travis Hoppe and
                Charles Foster and
                Jason Phang and
                Horace He and
                Anish Thite and
                Noa Nabeshima and
                Shawn Presser and
                Connor Leahy},
  title      = {The Pile: An 800GB Dataset of Diverse Text for Language Modeling},
  journal    = {CoRR},
  volume     = {abs/2101.00027},
  year       = {2021},
  url        = {https://arxiv.org/abs/2101.00027},
  eprinttype = {arXiv},
  eprint     = {2101.00027},
  timestamp  = {Thu, 14 Oct 2021 09:16:12 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2101-00027.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}


@article{sphere_corpus,
  author     = {Aleksandra Piktus and Fabio Petroni
                and Vladimir Karpukhin and Dmytro Okhonko
                and Samuel Broscheit and Gautier Izacard
                and Patrick Lewis and Barlas Oguz
                and Edouard Grave and Wen{-}tau Yih
                and Sebastian Riedel},
  title      = {The Web Is Your Oyster - Knowledge-Intensive {NLP} against a Very
                Large Web Corpus},
  journal    = {CoRR},
  volume     = {abs/2112.09924},
  year       = {2021},
  url        = {https://arxiv.org/abs/2112.09924},
  eprinttype = {arXiv},
  eprint     = {2112.09924},
  timestamp  = {Tue, 04 Jan 2022 15:59:27 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2112-09924.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{broad_twitter,
  title     = {Broad {T}witter Corpus: A Diverse Named Entity Recognition Resource},
  author    = {Derczynski, Leon  and
               Bontcheva, Kalina  and
               Roberts, Ian},
  booktitle = {Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers},
  month     = dec,
  year      = {2016},
  address   = {Osaka, Japan},
  publisher = {The COLING 2016 Organizing Committee},
  url       = {https://aclanthology.org/C16-1111},
  pages     = {1169--1179},
  abstract  = {One of the main obstacles, hampering method development and comparative evaluation of named entity recognition in social media, is the lack of a sizeable, diverse, high quality annotated corpus, analogous to the CoNLL{'}2003 news dataset. For instance, the biggest Ritter tweet corpus is only 45,000 tokens {--} a mere 15{\%} the size of CoNLL{'}2003. Another major shortcoming is the lack of temporal, geographic, and author diversity. This paper introduces the Broad Twitter Corpus (BTC), which is not only significantly bigger, but sampled across different regions, temporal periods, and types of Twitter users. The gold-standard named entity annotations are made by a combination of NLP experts and crowd workers, which enables us to harness crowd recall while maintaining high quality. We also measure the entity drift observed in our dataset (i.e. how entity representation varies over time), and compare to newswire. The corpus is released openly, including source text and intermediate annotations.}
}



@misc{brown_gpt3_2020,
  title     = {Language {Models} are {Few}-{Shot} {Learners}},
  url       = {http://arxiv.org/abs/2005.14165},
  doi       = {10.48550/arXiv.2005.14165},
  abstract  = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  urldate   = {2023-02-10},
  publisher = {arXiv},
  author    = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  month     = jul,
  year      = {2020},
  note      = {arXiv:2005.14165 [cs]},
  keywords  = {Computer Science - Computation and Language},
  annote    = {Comment: 40+32 pages},
  file      = {arXiv Fulltext PDF:/home/matey/Zotero/storage/PXVBS5HA/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf:application/pdf;arXiv.org Snapshot:/home/matey/Zotero/storage/S4EAQKRN/2005.html:text/html}
}


@misc{zhang_opt_2022,
  title      = {{OPT}: {Open} {Pre}-trained {Transformer} {Language} {Models}},
  shorttitle = {{OPT}},
  url        = {http://arxiv.org/abs/2205.01068},
  doi        = {10.48550/arXiv.2205.01068},
  abstract   = {Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.},
  urldate    = {2023-02-10},
  publisher  = {arXiv},
  author     = {Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and Mihaylov, Todor and Ott, Myle and Shleifer, Sam and Shuster, Kurt and Simig, Daniel and Koura, Punit Singh and Sridhar, Anjali and Wang, Tianlu and Zettlemoyer, Luke},
  month      = jun,
  year       = {2022},
  note       = {arXiv:2205.01068 [cs]},
  keywords   = {Computer Science - Computation and Language, Computer Science - Machine Learning},
  file       = {arXiv Fulltext PDF:/home/matey/Zotero/storage/KKLG6D68/Zhang et al. - 2022 - OPT Open Pre-trained Transformer Language Models.pdf:application/pdf;arXiv.org Snapshot:/home/matey/Zotero/storage/JSHMZHN7/2205.html:text/html}
}
